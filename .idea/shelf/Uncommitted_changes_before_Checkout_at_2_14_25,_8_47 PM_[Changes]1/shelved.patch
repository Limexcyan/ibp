Index: datasets.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport numpy as np\nimport torch\nfrom hypnettorch.data.special import permuted_mnist\nfrom hypnettorch.data.special.split_cifar import SplitCIFAR100Data\nfrom hypnettorch.data.special.split_mnist import get_split_mnist_handlers\nfrom TinyImageNet import TinyImageNet\nfrom CIFAR100_FeCAM import SplitCIFAR100Data_FeCAM\nimport attacks\n\ndef generate_random_permutations(\n    shape_of_data_instance, number_of_permutations\n):\n    \"\"\"\n    Prepare a list of random permutations of the selected shape\n    for continual learning tasks.\n\n    Arguments:\n    ----------\n      *shape_of_data_instance*: a number defining shape of the dataset\n      *number_of_permutations*: int, a number of permutations that will\n                                be prepared; it corresponds to the total\n                                number of tasks\n      *seed*: int, optional argument, default: None\n              if one would get deterministic results\n    \"\"\"\n    list_of_permutations = []\n    for _ in range(number_of_permutations):\n        list_of_permutations.append(\n            np.random.permutation(shape_of_data_instance)\n        )\n    return list_of_permutations\n\n\ndef prepare_split_cifar100_tasks(\n    datasets_folder, validation_size, use_augmentation, use_cutout=False\n):\n    \"\"\"\n    Prepare a list of 10 tasks with 10 classes per each task.\n    i-th task, where i in {0, 1, ..., 9} will store samples\n    from classes {10*i, 10*i + 1, ..., 10*i + 9}.\n\n    Arguments:\n    ----------\n      *datasets_folder*: (string) Defines a path in which CIFAR-100\n                         is stored / will be downloaded\n      *validation_size*: (int) The number of validation samples\n      *use_augmentation*: (Boolean) potentially applies\n                          a data augmentation method from\n                          hypnettorch\n      *use_cutout*: (optional Boolean) in the positive case it applies\n                    'apply_cutout' option form 'torch_input_transforms'.\n    \"\"\"\n    handlers = []\n    for i in range(0, 100, 10):\n        handlers.append(\n            SplitCIFAR100Data(\n                datasets_folder,\n                use_one_hot=True,\n                validation_size=validation_size,\n                use_data_augmentation=use_augmentation,\n                use_cutout=use_cutout,\n                labels=range(i, i + 10),\n            )\n        )\n    return handlers\n\n\ndef prepare_split_cifar100_tasks_aka_FeCAM(\n    datasets_folder,\n    number_of_tasks,\n    no_of_validation_samples_per_class,\n    use_augmentation,\n    use_cutout=False,\n):\n    \"\"\"\n    Prepare a list of 5, 10 or 20 incremental tasks with 20, 10 or 5 classes,\n    respectively, per each task. Furthermore, the first task contains\n    a higher number of classes, i.e. 50 or 40. Therefore, in these cases,\n    the total number of tasks is equal to 6, 11 or 21.\n    Also, there is a possibility of 5 tasks with 20 classes per each.\n    The order of classes is the same like in FeCAM, also the scenarios\n    are constructed in such a way to enable a fair comparison with FeCAM\n\n    Arguments:\n    ----------\n      *datasets_folder*: (string) Defines a path in which CIFAR-100\n                         is stored / will be downloaded\n      *number_of_tasks* (int) Defines how many continual learning tasks\n                        will be created. Possible options: 6, 11 or 21\n      *no_of_validation_samples_per_class*: (int) The number of validation\n                                            samples in a single class\n      *use_augmentation*: (Boolean) potentially applies\n                          a data augmentation method from\n                          hypnettorch\n      *use_cutout*: (optional Boolean) in the positive case it applies\n                    'apply_cutout' option form 'torch_input_transforms'.\n    \"\"\"\n    # FeCAM considered four scenarios: 5, 10 and 20 incremental tasks\n    # and 5 tasks with the equal number of classes\n    assert number_of_tasks in [5, 6, 11, 21]\n    # The order of image classes in the case of FeCAM was not 0-10, 11-20, etc.,\n    # but it was chosen randomly by the authors, and was at follows:\n    class_orders = [\n        87, 0, 52, 58, 44, 91, 68, 97, 51, 15,\n        94, 92, 10, 72, 49, 78, 61, 14, 8, 86,\n        84, 96, 18, 24, 32, 45, 88, 11, 4, 67,\n        69, 66, 77, 47, 79, 93, 29, 50, 57, 83,\n        17, 81, 41, 12, 37, 59, 25, 20, 80, 73,\n        1, 28, 6, 46, 62, 82, 53, 9, 31, 75,\n        38, 63, 33, 74, 27, 22, 36, 3, 16, 21,\n        60, 19, 70, 90, 89, 43, 5, 42, 65, 76,\n        40, 30, 23, 85, 2, 95, 56, 48, 71, 64,\n        98, 13, 99, 7, 34, 55, 54, 26, 35, 39\n    ]\n    # Incremental tasks from Table I, FeCAM\n    if number_of_tasks == 6:\n        numbers_of_classes_per_tasks = [50]\n        numbers_of_classes_per_tasks.extend([10 for i in range(5)])\n    elif number_of_tasks == 11:\n        numbers_of_classes_per_tasks = [50]\n        numbers_of_classes_per_tasks.extend([5 for i in range(10)])\n    elif number_of_tasks == 21:\n        numbers_of_classes_per_tasks = [40]\n        numbers_of_classes_per_tasks.extend([3 for i in range(20)])\n    # Tasks with the equal number of elements, Table V, FeCAM\n    elif number_of_tasks == 5:\n        numbers_of_classes_per_tasks = [20 for i in range(5)]\n\n    handlers = []\n    for i in range(len(numbers_of_classes_per_tasks)):\n        current_number_of_tasks = numbers_of_classes_per_tasks[i]\n        validation_size = (\n            no_of_validation_samples_per_class * current_number_of_tasks\n        )\n        handlers.append(\n            SplitCIFAR100Data_FeCAM(\n                datasets_folder,\n                use_one_hot=True,\n                validation_size=validation_size,\n                use_data_augmentation=use_augmentation,\n                use_cutout=use_cutout,\n                labels=class_orders[\n                    (i * current_number_of_tasks) : (\n                        (i + 1) * current_number_of_tasks\n                    )\n                ],\n            )\n        )\n    return handlers\n\n\ndef prepare_tinyimagenet_tasks(\n    datasets_folder, seed, validation_size=250, number_of_tasks=40\n):\n    \"\"\"\n    Prepare a list of *number_of_tasks* tasks related\n    to the TinyImageNet dataset according to the WSN setup.\n\n    Arguments:\n    ----------\n      *datasets_folder*: (string) Defines a path in which TinyImageNet\n                         is stored / will be downloaded\n      *seed*: (int) Necessary for the preparation of random permutation\n              of the order of classes in consecutive tasks.\n      *validation_size*: (optional int) defines the number of validation\n                         samples in each task, by default it is 250 like\n                         in the case of WSN\n      *number_of_tasks*: (optional int) defines the number of continual\n                         learning tasks (by default: 40)\n\n    Returns a list of TinyImageNet objects.\n    \"\"\"\n    # Set randomly the order of classes\n    rng = np.random.default_rng(seed)\n    class_permutation = rng.permutation(200)\n    # 40 classification tasks with 5 classes in each\n    handlers = []\n    for i in range(0, 5 * number_of_tasks, 5):\n        current_labels = class_permutation[i : (i + 5)]\n        print(f\"Order of classes in the current task: {current_labels}\")\n        handlers.append(\n            TinyImageNet(\n                data_path=datasets_folder,\n                validation_size=validation_size,\n                use_one_hot=True,\n                labels=current_labels,\n            )\n        )\n    return handlers\n\n\ndef prepare_permuted_mnist_tasks(\n    datasets_folder, input_shape, number_of_tasks, padding, validation_size\n):\n    \"\"\"\n    Prepare a list of *number_of_tasks* tasks related\n    to the PermutedMNIST dataset.\n\n    Arguments:\n    ----------\n      *datasets_folder*: (string) Defines a path in which MNIST dataset\n                         is stored / will be downloaded\n      *input_shape*: (int) a number defining shape of the dataset\n      *validation_size*: (int) The number of validation samples\n\n    Returns a list of PermutedMNIST objects.\n    \"\"\"\n    permutations = generate_random_permutations(input_shape, number_of_tasks)\n    return permuted_mnist.PermutedMNISTList(\n        permutations,\n        datasets_folder,\n        use_one_hot=True,\n        padding=padding,\n        validation_size=validation_size,\n    )\n\n\ndef prepare_split_mnist_tasks(\n    datasets_folder, validation_size, use_augmentation, number_of_tasks=5\n):\n    \"\"\"\n    Prepare a list of *number_of_tasks* tasks related\n    to the SplitMNIST dataset. By default, it should be\n    5 task containing consecutive pairs of classes:\n    [0, 1], [2, 3], [4, 5], [6, 7] and [8, 9].\n\n    Arguments:\n    ----------\n      *datasets_folder*: (string) Defines a path in which MNIST dataset\n                         is stored / will be downloaded\n      *validation_size*: (int) The number of validation samples\n      *use_augmentation*: (bool) defines whether dataset augmentation\n                          will be applied\n      *number_of_tasks* (int) a number defining the number of learning\n                        tasks, by default 5.\n\n    Returns a list of SplitMNIST objects.\n    \"\"\"\n    return get_split_mnist_handlers(\n        datasets_folder,\n        use_one_hot=True,\n        validation_size=validation_size,\n        num_classes_per_task=2,\n        num_tasks=number_of_tasks,\n        use_torch_augmentation=use_augmentation,\n    )\n\n\ndef set_hyperparameters(dataset, grid_search=False, part=0):\n    \"\"\"\n    Set hyperparameters of the experiments, both in the case of grid search\n    optimization and a single network run.\n\n    Arguments:\n    ----------\n      *dataset*: 'PermutedMNIST', 'SplitMNIST' or 'CIFAR100'\n      *grid_search*: (Boolean optional) defines whether a hyperparameter\n                     optimization should be performed or hyperparameters\n                     for just a single run have to be returned\n      *part* (only for SplitMNIST or CIFAR100!) selects a subset\n             of hyperparameters for optimization (by default 0)\n\n    Returns a dictionary with necessary hyperparameters.\n    \"\"\"\n    if dataset == \"PermutedMNIST\":\n        # us or paper\n        param_owner = 'paper'\n        if param_owner == 'us':\n            if grid_search:\n                hyperparams = {\n                    \"embedding_sizes\": [24],\n                    \"learning_rates\": [0.001],\n                    \"batch_sizes\": [128],\n                    \"norm_regularizer_masking_opts\": [True, False],\n                    \"betas\": [0.001, 0.0005, 0.005],\n                    \"hypernetworks_hidden_layers\": [[100, 100]],\n                    \"sparsity_parameters\": [0],\n                    \"lambdas\": [0.001, 0.0005],\n                    \"best_model_selection_method\": \"val_loss\",\n                    \"saving_folder\": \"./Results/grid_search/permuted_mnist/\",\n                    # not for optimization, just for multiple cases\n                    \"seed\": [1, 2, 3, 4, 5],\n                }\n\n            else:\n                # Best hyperparameters\n                hyperparams = {\n                    \"seed\": [1, 2, 3, 4, 5],\n                    \"embedding_sizes\": [24],\n                    \"sparsity_parameters\": [0],\n                    \"learning_rates\": [0.001],\n                    \"batch_sizes\": [128],\n                    \"betas\": [0.0005],\n                    \"lambdas\": [0.001],\n                    \"norm_regularizer_masking_opts\": [True],\n                    \"hypernetworks_hidden_layers\": [[100, 100]],\n                    \"best_model_selection_method\": \"last_model\",\n                    \"saving_folder\": \"./Results/permuted_mnist_best_hyperparams/\",\n                }\n\n            # Both in the grid search and individual runs\n            hyperparams[\"lr_scheduler\"] = False\n            hyperparams[\"number_of_iterations\"] = 5000\n            hyperparams[\"number_of_epochs\"] = None\n            hyperparams[\"no_of_validation_samples\"] = 5000\n            hyperparams[\"no_of_validation_samples_per_class\"] = 500\n            hyperparams[\"target_hidden_layers\"] = [1000, 1000]\n            hyperparams[\"target_network\"] = \"epsMLP\"\n            hyperparams[\"resnet_number_of_layer_groups\"] = None\n            hyperparams[\"resnet_widening_factor\"] = None\n            hyperparams[\"optimizer\"] = \"adam\"\n            hyperparams[\"chunk_size\"] = 100\n            hyperparams[\"chunk_emb_size\"] = 8\n            hyperparams[\"use_chunks\"] = False\n            hyperparams[\"adaptive_sparsity\"] = True\n            hyperparams[\"use_batch_norm\"] = False\n            # Directly related to the MNIST dataset\n            hyperparams[\"padding\"] = 2\n            hyperparams[\"shape\"] = (28 + 2 * hyperparams[\"padding\"]) ** 2\n            hyperparams[\"number_of_tasks\"] = 10\n            hyperparams[\"augmentation\"] = False\n        # paper\n        else:\n            if grid_search:\n                hyperparams = {\n                    \"embedding_sizes\": [24],\n                    \"learning_rates\": [0.001],\n                    \"batch_sizes\": [32],\n                    \"norm_regularizer_masking_opts\": [True, False],\n                    \"betas\": [0.001, 0.0005, 0.005, 0.0001, 0.01, 0.05],\n                    \"hypernetworks_hidden_layers\": [[100, 100]],\n                    \"sparsity_parameters\": [0],\n                    \"lambdas\": [0.001, 0.0005, 0.0001, 0.005, 0.01, 0.05],\n                    \"best_model_selection_method\": \"val_loss\",\n                    \"saving_folder\": \"./Results/grid_search/permuted_mnist/\",\n                    # not for optimization, just for multiple cases\n                    \"seed\": [1, 2, 3, 4, 5],\n                }\n\n            else:\n                # Best hyperparameters\n                hyperparams = {\n                    \"seed\": [1, 2, 3, 4, 5],\n                    \"embedding_sizes\": [24],\n                    \"sparsity_parameters\": [0],\n                    \"learning_rates\": [0.001],\n                    \"batch_sizes\": [32],\n                    \"betas\": [0.0005],\n                    \"lambdas\": [0.001],\n                    \"norm_regularizer_masking_opts\": [True],\n                    \"hypernetworks_hidden_layers\": [[100,100]],\n                    \"best_model_selection_method\": \"last_model\",\n                    \"saving_folder\": \"./Results/permuted_mnist_best_hyperparams/\",\n                }\n\n            # Both in the grid search and individual runs\n            hyperparams[\"lr_scheduler\"] = False\n            hyperparams[\"number_of_iterations\"] = 10000\n            hyperparams[\"number_of_epochs\"] = 10\n            hyperparams[\"no_of_validation_samples\"] = 5000\n            hyperparams[\"no_of_validation_samples_per_class\"] = 500\n            hyperparams[\"target_hidden_layers\"] = [256,256, 10]\n            hyperparams[\"target_network\"] = \"epsMLP\"\n            hyperparams[\"resnet_number_of_layer_groups\"] = None\n            hyperparams[\"resnet_widening_factor\"] = None\n            hyperparams[\"optimizer\"] = \"adam\"\n            hyperparams[\"chunk_size\"] = 100\n            hyperparams[\"chunk_emb_size\"] = 8\n            hyperparams[\"use_chunks\"] = False\n            hyperparams[\"adaptive_sparsity\"] = True\n            hyperparams[\"use_batch_norm\"] = False\n            # Directly related to the MNIST dataset\n            hyperparams[\"padding\"] = 2\n            hyperparams[\"shape\"] = (28 + 2 * hyperparams[\"padding\"]) ** 2\n            hyperparams[\"number_of_tasks\"] = 10\n            hyperparams[\"augmentation\"] = False\n\n    elif dataset == \"CIFAR100\":\n        if grid_search:\n            hyperparams = {\n                \"seed\": [5],\n                \"sparsity_parameters\": [0],\n                \"embedding_sizes\": [48],\n                \"betas\": [0.01, 0.05, 0.1, 1],\n                \"lambdas\": [0.01, 0.1, 1],\n                \"learning_rates\": [0.001],\n                \"batch_sizes\": [32],\n                \"norm_regularizer_masking_opts\": [False],\n                \"hypernetworks_hidden_layers\": [[100]],\n                \"resnet_number_of_layer_groups\": 3,\n                \"resnet_widening_factor\": 2,\n                \"optimizer\": \"adam\",\n                \"use_batch_norm\": True,\n                \"target_network\": \"ResNet\",\n                \"use_chunks\": False,\n                \"number_of_epochs\": 200,\n                \"augmentation\": True,\n            }\n            if part == 0:\n                pass\n            elif part == 1:\n                hyperparams[\"target_network\"] = \"ZenkeNet\"\n                hyperparams[\"resnet_number_of_layer_groups\"] = None\n                hyperparams[\"resnet_widening_factor\"] = None\n                hyperparams[\"use_chunks\"] = False\n                hyperparams[\"use_batch_norm\"] = False\n            else:\n                raise ValueError(f\"Wrong argument: {part}!\")\n            hyperparams[\n                \"saving_folder\"\n            ] = f\"./Results/grid_search/CIFAR_100_part_{part}/\"\n\n        else:\n            # Best hyperparameters for ResNet\n            hyperparams = {\n                \"seed\": [1, 2, 3, 4, 5],\n                \"embedding_sizes\": [48],\n                \"sparsity_parameters\": [0],\n                \"betas\": [0.01],\n                \"lambdas\": [1],\n                \"batch_sizes\": [32],\n                \"learning_rates\": [0.001],\n                \"norm_regularizer_masking_opts\": [False],\n                \"hypernetworks_hidden_layers\": [[100]],\n                \"use_batch_norm\": True,\n                \"use_chunks\": False,\n                \"resnet_number_of_layer_groups\": 3,\n                \"resnet_widening_factor\": 2,\n                \"number_of_epochs\": 200,\n                \"target_network\": \"ResNet\",\n                \"optimizer\": \"adam\",\n                \"augmentation\": True,\n            }\n            if part == 0:\n                # ResNet\n                pass\n            elif part == 1:\n                # ZenkeNet\n                hyperparams[\"lambdas\"] = [0.01]\n                hyperparams[\"target_network\"] = \"ZenkeNet\"\n                hyperparams[\"resnet_number_of_layer_groups\"] = None\n                hyperparams[\"resnet_widening_factor\"] = None\n            else:\n                raise ValueError(f\"Wrong argument: {part}!\")\n            hyperparams[\n                \"saving_folder\"\n            ] = f\"./Results/CIFAR_100_best_hyperparams_part_{part}/\"\n        hyperparams[\"lr_scheduler\"] = True\n        hyperparams[\"number_of_iterations\"] = None\n        hyperparams[\"no_of_validation_samples\"] = 500\n        hyperparams[\"no_of_validation_samples_per_class\"] = 50\n        if hyperparams[\"target_network\"] in [\"ResNet\", \"ZenkeNet\"]:\n            hyperparams[\"shape\"] = 32\n            hyperparams[\"target_hidden_layers\"] = None\n        elif hyperparams[\"target_network\"] == \"MLP\":\n            hyperparams[\"shape\"] = 3072\n            hyperparams[\"target_hidden_layers\"] = [1000, 1000]\n        hyperparams[\"number_of_tasks\"] = 10\n        hyperparams[\"chunk_size\"] = 100\n        hyperparams[\"chunk_emb_size\"] = 32\n        hyperparams[\"adaptive_sparsity\"] = True\n        hyperparams[\"padding\"] = None\n        hyperparams[\"best_model_selection_method\"] = \"val_loss\"\n\n    elif dataset == \"TinyImageNet\":\n        if grid_search:\n            hyperparams = {\n                \"seed\": [5],\n                \"sparsity_parameters\": [0, 30],\n                \"embedding_sizes\": [48],\n                \"betas\": [0.01, 0.1],\n                \"lambdas\": [0.01, 0.1],\n                \"learning_rates\": [0.001],\n                \"batch_sizes\": [16],\n                \"norm_regularizer_masking_opts\": [False],\n                \"hypernetworks_hidden_layers\": [[10, 10], [100]],\n                \"resnet_number_of_layer_groups\": 3,\n                \"resnet_widening_factor\": 2,\n                \"optimizer\": \"adam\",\n                \"use_batch_norm\": True,\n                \"target_network\": \"ResNet\",\n                \"use_chunks\": False,\n                \"number_of_epochs\": 10,\n                \"augmentation\": True,\n                \"saving_folder\": f\"./Results/TinyImageNet_grid_search_part_{part}/\",\n            }\n            if part == 0:\n                pass\n            elif part in [1, 2, 3]:\n                # ZenkeNet\n                hyperparams[\"seed\"] = [6]\n                hyperparams[\"target_network\"] = \"ZenkeNet\"\n                hyperparams[\"resnet_number_of_layer_groups\"] = None\n                hyperparams[\"resnet_widening_factor\"] = None\n                hyperparams[\"use_batch_norm\"] = False\n                hyperparams[\"hypernetworks_hidden_layers\"] = [[100, 100]]\n                hyperparams[\"learning_rates\"] = [0.001]\n                if part == 1:\n                    hyperparams[\"betas\"] = [0.01, 0.1, 1.0]\n                    hyperparams[\"lambdas\"] = [0.01, 0.1, 1.0]\n                    hyperparams[\"sparsity_parameters\"] = [0, 50, 70]\n                    hyperparams[\"embedding_sizes\"] = [96]\n                    hyperparams[\"learning_rates\"] = [0.005]\n                elif part == 2:\n                    hyperparams[\"embedding_sizes\"] = [128]\n                    hyperparams[\"betas\"] = [0.01, 0.1, 1.0]\n                    hyperparams[\"lambdas\"] = [0.01, 0.1, 1.0]\n                    hyperparams[\"sparsity_parameters\"] = [0, 30, 50]\n                elif part == 3:\n                    hyperparams[\"embedding_sizes\"] = [192]\n                    hyperparams[\"sparsity_parameters\"] = [0, 50]\n                    hyperparams[\"betas\"] = [0.001, 0.01, 0.1, 1]\n                    hyperparams[\"lambdas\"] = [0.001, 0.01, 0.1, 1]\n            else:\n                raise ValueError(f\"Wrong argument: {part}!\")\n            hyperparams[\n                \"saving_folder\"\n            ] = f\"./Results/Tiny_Zenke_grid_search_part_{part}/\"\n        else:\n            # ResNet\n            hyperparams = {\n                \"seed\": [5, 6, 7, 8, 9],\n                \"embedding_sizes\": [96],\n                \"sparsity_parameters\": [0],\n                \"betas\": [1],\n                \"lambdas\": [0.1],\n                \"batch_sizes\": [16],\n                \"learning_rates\": [0.0001],\n                \"norm_regularizer_masking_opts\": [False],\n                \"hypernetworks_hidden_layers\": [[100, 100]],\n                \"use_batch_norm\": True,\n                \"use_chunks\": False,\n                \"resnet_number_of_layer_groups\": 3,\n                \"resnet_widening_factor\": 2,\n                \"number_of_epochs\": 10,\n                \"target_network\": \"ResNet\",\n                \"optimizer\": \"adam\",\n                \"augmentation\": True,\n                \"saving_folder\": \"./Results/TinyImageNet/ResNet_best_hyperparams/\",\n            }\n            if part == 0:\n                pass\n            # ZenkeNet\n            elif part == 1:\n                hyperparams[\"target_network\"] = \"ZenkeNet\"\n                hyperparams[\"resnet_number_of_layer_groups\"] = None\n                hyperparams[\"resnet_widening_factor\"] = None\n                hyperparams[\"use_batch_norm\"] = False\n                hyperparams[\"sparsity_parameters\"] = [50]\n                hyperparams[\"betas\"] = [0.01]\n                hyperparams[\"lambdas\"] = [1.0]\n                hyperparams[\"learning_rates\"] = [0.001]\n                hyperparams[\n                    \"saving_folder\"\n                ] = f\"./Results/TinyImageNet/ZenkeNet_best_hyperparams/\"\n            else:\n                raise ValueError(f\"Wrong argument: {part}!\")\n        hyperparams[\"lr_scheduler\"] = True\n        hyperparams[\"number_of_iterations\"] = None\n        hyperparams[\"no_of_validation_samples\"] = 250\n        hyperparams[\"no_of_validation_samples_per_class\"] = 50\n        if hyperparams[\"target_network\"] in [\"ResNet\", \"ZenkeNet\"]:\n            hyperparams[\"shape\"] = 64\n            hyperparams[\"target_hidden_layers\"] = None\n        elif hyperparams[\"target_network\"] == \"MLP\":\n            hyperparams[\"shape\"] = 12288\n            hyperparams[\"target_hidden_layers\"] = [1000, 1000]\n        hyperparams[\"number_of_tasks\"] = 40\n        hyperparams[\"chunk_size\"] = 100\n        hyperparams[\"chunk_emb_size\"] = 32\n        hyperparams[\"adaptive_sparsity\"] = True\n        hyperparams[\"padding\"] = None\n        hyperparams[\"best_model_selection_method\"] = \"val_loss\"\n\n    elif dataset == \"SplitMNIST\":\n        if grid_search:\n            hyperparams = {\n                \"learning_rates\": [0.001],\n                \"batch_sizes\": [128],\n                \"norm_regularizer_masking_opts\": [False],\n                \"betas\": [0.001],\n                \"hypernetworks_hidden_layers\": [[25, 25]],\n                \"sparsity_parameters\": [0],\n                \"lambdas\": [0.001],\n                # Seed is not for optimization but for ensuring multiple results\n                \"seed\": [1, 2, 3, 4, 5],\n                \"best_model_selection_method\": \"last_model\",\n                \"embedding_sizes\": [128],\n                \"augmentation\": True,\n            }\n            if part == 0:\n                pass\n            elif part == 1:\n                hyperparams[\"embedding_sizes\"] = [96]\n                hyperparams[\"hypernetworks_hidden_layers\"] = [[50, 50]]\n                hyperparams[\"betas\"] = [0.01]\n                hyperparams[\"sparsity_parameters\"] = [30]\n                hyperparams[\"lambdas\"] = [0.0001]\n            elif part == 2:\n                hyperparams[\"sparsity_parameters\"] = [30]\n                hyperparams[\"norm_regularizer_masking_opts\"] = [True]\n            else:\n                raise ValueError(\"Not implemented subset of hyperparameters!\")\n\n            hyperparams[\"saving_folder\"] = \"./Results/grid_search/split_mnist/\"\n\n        else:\n            # Best hyperparameters\n            hyperparams = {\n                \"seed\": [1],\n                \"embedding_sizes\": [128],\n                \"sparsity_parameters\": [30],\n                \"learning_rates\": [0.001],\n                \"batch_sizes\": [128],\n                \"betas\": [0.001],\n                \"lambdas\": [0.001],\n                \"norm_regularizer_masking_opts\": [True],\n                \"hypernetworks_hidden_layers\": [[25, 25]],\n                \"augmentation\": True,\n                \"best_model_selection_method\": \"last_model\",\n                \"saving_folder\": \"./Results/split_mnist_test/\",\n            }\n        hyperparams[\"lr_scheduler\"] = False\n        hyperparams[\"target_network\"] = \"epsMLP\"\n        hyperparams[\"resnet_number_of_layer_groups\"] = None\n        hyperparams[\"resnet_widening_factor\"] = None\n        hyperparams[\"optimizer\"] = \"adam\"\n        hyperparams[\"number_of_iterations\"] = 200\n        hyperparams[\"number_of_epochs\"] = None\n        hyperparams[\"no_of_validation_samples\"] = 1000\n        hyperparams[\"no_of_validation_samples_per_class\"] = 100\n        hyperparams[\"target_hidden_layers\"] = [400, 400]\n        hyperparams[\"shape\"] = 28**2\n        hyperparams[\"number_of_tasks\"] = 5\n        hyperparams[\"chunk_size\"] = 100\n        hyperparams[\"chunk_emb_size\"] = 96\n        hyperparams[\"use_chunks\"] = False\n        hyperparams[\"use_batch_norm\"] = False\n        hyperparams[\"adaptive_sparsity\"] = True\n        hyperparams[\"padding\"] = None\n\n    elif dataset == \"CIFAR100_FeCAM_setup\":\n        if grid_search:\n            hyperparams = {\n                \"seed\": [1],\n                \"sparsity_parameters\": [0],\n                \"betas\": [0.01],\n                \"lambdas\": [1],\n                \"batch_sizes\": [32],\n                \"norm_regularizer_masking_opts\": [False],\n                \"hypernetworks_hidden_layers\": [[100]],\n                \"use_batch_norm\": True,\n                \"use_chunks\": False,\n                \"resnet_number_of_layer_groups\": 3,\n                \"resnet_widening_factor\": 2,\n                \"number_of_epochs\": 200,\n                \"target_network\": \"ResNet\",\n                \"optimizer\": \"adam\",\n                \"augmentation\": True,\n            }\n            if part == 0:\n                pass\n            elif part == 1:\n                hyperparams[\"embedding_sizes\"] = [24, 48, 96]\n                hyperparams[\"learning_rates\"] = [0.0001, 0.001, 0.01]\n                hyperparams[\"hypernetworks_hidden_layers\"] = [[100], [200]]\n                hyperparams[\"number_of_tasks\"] = 5\n            hyperparams[\n                \"saving_folder\"\n            ] = f\"./Results/CIFAR_100_FeCAM_setup_part_{part}/\"\n        else:\n            # Best hyperparameters for ResNet\n            hyperparams = {\n                \"seed\": [1, 2, 3, 4, 5],\n                \"embedding_sizes\": [48],\n                \"sparsity_parameters\": [0],\n                \"betas\": [0.01],\n                \"lambdas\": [1],\n                \"batch_sizes\": [32],\n                \"learning_rates\": [0.0001],\n                \"norm_regularizer_masking_opts\": [False],\n                \"hypernetworks_hidden_layers\": [[200]],\n                \"use_batch_norm\": True,\n                \"use_chunks\": False,\n                \"resnet_number_of_layer_groups\": 3,\n                \"resnet_widening_factor\": 2,\n                \"number_of_epochs\": 200,\n                \"target_network\": \"ResNet\",\n                \"optimizer\": \"adam\",\n                \"augmentation\": True,\n            }\n            # FeCAM considered three incremental scenarios: with 6, 11 and 21 tasks\n            # ResNet - parts 0, 1 and 2\n            # ZenkeNet - parts 3, 4 and 5\n            # Also, one scenario with equal number of classes: ResNet - part 6\n            if part in [0, 3]:\n                hyperparams[\"number_of_tasks\"] = 6\n            elif part in [1, 4]:\n                hyperparams[\"number_of_tasks\"] = 11\n            elif part in [2, 5]:\n                hyperparams[\"number_of_tasks\"] = 21\n            elif part in [6, 7]:\n                hyperparams[\"number_of_tasks\"] = 5\n            if part in [3, 4, 5, 7]:\n                hyperparams[\"lambdas\"] = [0.01]\n                hyperparams[\"target_network\"] = \"ZenkeNet\"\n                hyperparams[\"resnet_number_of_layer_groups\"] = None\n                hyperparams[\"resnet_widening_factor\"] = None\n            if part not in [0, 1, 2, 3, 4, 5, 6, 7]:\n                raise ValueError(f\"Wrong argument: {part}!\")\n            hyperparams[\n                \"saving_folder\"\n            ] = f\"./Results/CIFAR_100_FeCAM_part_{part}/\"\n        hyperparams[\"lr_scheduler\"] = True\n        hyperparams[\"number_of_iterations\"] = None\n        hyperparams[\"no_of_validation_samples_per_class\"] = 50\n        if hyperparams[\"target_network\"] in [\"ResNet\", \"ResNetF\", \"ZenkeNet\"]:\n            hyperparams[\"shape\"] = 32\n            hyperparams[\"target_hidden_layers\"] = None\n        elif hyperparams[\"target_network\"] == \"MLP\":\n            hyperparams[\"shape\"] = 3072\n            hyperparams[\"target_hidden_layers\"] = [1000, 1000]\n        hyperparams[\"chunk_size\"] = 100\n        hyperparams[\"chunk_emb_size\"] = 32\n        hyperparams[\"adaptive_sparsity\"] = True\n        hyperparams[\"padding\"] = None\n        hyperparams[\"best_model_selection_method\"] = \"val_loss\"\n\n    else:\n        raise ValueError(\"This dataset is not implemented!\")\n\n    # General hyperparameters\n    hyperparams[\"activation_function\"] = torch.nn.ELU()\n    hyperparams[\"norm\"] = 1  # L1 norm\n    hyperparams[\"use_bias\"] = True\n    hyperparams[\"save_consecutive_masks\"] = False\n    hyperparams[\"device\"] = \"cpu\"\n    hyperparams[\"dataset\"] = dataset\n    os.makedirs(hyperparams[\"saving_folder\"], exist_ok=True)\n    return hyperparams\n\n\nif __name__ == \"__main__\":\n    datasets_folder = \"./Data\"\n    os.makedirs(datasets_folder, exist_ok=True)\n    validation_size = 500\n    use_data_augmentation = False\n    use_cutout = False\n\n    split_cifar100_list = prepare_split_cifar100_tasks(\n        datasets_folder, validation_size, use_data_augmentation, use_cutout\n    )\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/datasets.py b/datasets.py
--- a/datasets.py	(revision 9bcf188f17f73fe837356539a1ef3df5f2cd5485)
+++ b/datasets.py	(date 1739549717385)
@@ -622,7 +622,7 @@
                 "saving_folder": "./Results/split_mnist_test/",
             }
         hyperparams["lr_scheduler"] = False
-        hyperparams["target_network"] = "epsMLP"
+        hyperparams["target_network"] = "MLP"
         hyperparams["resnet_number_of_layer_groups"] = None
         hyperparams["resnet_widening_factor"] = None
         hyperparams["optimizer"] = "adam"
