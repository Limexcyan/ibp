Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport random\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch.optim as optim\nfrom hypnettorch.mnets import MLP\nfrom hypnettorch.mnets.resnet import ResNet\nfrom epsMLP import epsMLP\n\nfrom ZenkeNet64 import ZenkeNet\nfrom hypnettorch.hnets import HMLP\nfrom hypnettorch.hnets.chunked_mlp_hnet import ChunkedHMLP\nimport hypnettorch.utils.hnet_regularizer as hreg\nfrom torch import nn\nfrom datetime import datetime\nfrom itertools import product\nfrom copy import deepcopy\nfrom retry import retry\n\nfrom torchattacks import FGSM, PGD, AutoAttack\n\nfrom datasets import (\n    set_hyperparameters,\n    prepare_split_cifar100_tasks,\n    prepare_split_cifar100_tasks_aka_FeCAM,\n    prepare_permuted_mnist_tasks,\n    prepare_split_mnist_tasks,\n    prepare_tinyimagenet_tasks,\n)\n\nfrom autoattack import AutoAttack\n\ndef set_seed(value):\n    \"\"\"\n    Set deterministic results according to the given value\n    (including random, numpy and torch libraries)\n    \"\"\"\n    random.seed(value)\n    np.random.seed(value)\n    torch.manual_seed(value)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef append_row_to_file(filename, elements):\n    \"\"\"\n    Append a single row to the given file.\n\n    Parameters\n    ----------\n    filename: folder and name of file\n    elements: elements to saving in filename\n    \"\"\"\n    if not filename.endswith(\".csv\"):\n        filename += \".csv\"\n    filename = filename.replace(\".pt\", \"\")\n    with open(filename, \"a+\") as stream:\n        np.savetxt(\n            stream, np.array(elements)[np.newaxis], delimiter=\";\", fmt=\"%s\"\n        )\n\n\ndef write_pickle_file(filename, object_to_save):\n    torch.save(object_to_save, f\"{filename}.pt\")\n\n\n@retry((OSError, IOError))\ndef load_pickle_file(filename):\n    return torch.load(filename)\n\n\ndef get_shapes_of_network(model):\n    \"\"\"\n    Get shape of all layers of the loaded model.\n\n    Argument:\n    ---------\n      *model*: an instance of hypnettorch model, e.g. MLP from mnets\n\n    Returns:\n    --------\n      A list with lists of shapes of consecutive network layers\n    \"\"\"\n    shapes_of_model = []\n    for layer in model.weights:\n        shapes_of_model.append(list(layer.shape))\n    return shapes_of_model\n\n\ndef calculate_number_of_iterations(\n    number_of_samples, batch_size, number_of_epochs\n):\n    \"\"\"\n    Calculate the total number of iterations based on the number\n    of samples, desired batch size and number of training epochs.\n\n    Arguments:\n    ----------\n      *number_of_samples* (int) a number of individual samples\n      *batch_size* (int) a number of samples entering the network\n                   at one iteration\n      *number_of_epochs* (int) a desired number of training epochs\n\n    Returns:\n    --------\n      *no_of_iterations_per_epoch* (int) a number of training iterations\n                                   per one epoch\n      *total_no_of_iterations* (int) a total number of training iterations\n    \"\"\"\n    no_of_iterations_per_epoch = int(np.ceil(number_of_samples / batch_size))\n    total_no_of_iterations = int(no_of_iterations_per_epoch * number_of_epochs)\n    return no_of_iterations_per_epoch, total_no_of_iterations\n\ndef calculate_accuracy(data, target_network, weights, parameters, evaluation_dataset):\n    \"\"\"\n    Calculate accuracy for a given dataset using a selected network\n    and a selected set of weights. Optionally applies FGSM or PGD attack.\n\n    Arguments:\n    ----------\n      *data*: Dataset instance (e.g., hypnettorch.data.special.permuted_mnist.PermutedMNIST)\n              in the case of the PermutedMNIST dataset.\n      *target_network*: An instance of the network to be evaluated.\n      *weights*: Weights for the *target_network* network\n                 (an instance of torch.nn.modules.container.ParameterList).\n      *parameters*: Dictionary containing the following keys:\n        - 'device': (string) 'cuda' or 'cpu', determines the computation device.\n        - 'use_batch_norm_memory': (boolean) Whether to use stored weights for batch\n                                   normalization layers. If True, 'number_of_task'\n                                   must also be provided.\n        - 'number_of_task': (int or None) The task index currently being solved.\n                            Required if 'use_batch_norm_memory' is True.\n      *evaluation_dataset*: (string) Either 'validation' or 'test', specifies the dataset\n                            to evaluate.\n\n    Returns:\n    --------\n      torch.Tensor: Accuracy (percentage) for the selected setting.\n    \"\"\"\n    assert (\n        parameters[\"use_batch_norm_memory\"]\n        and parameters[\"number_of_task\"]is not None\n    ) or not parameters[\"use_batch_norm_memory\"]\n    assert evaluation_dataset in [\"validation\", \"test\"]\n    target_network.eval()\n    torch.no_grad()\n    if evaluation_dataset == \"validation\":\n        input_data = data.get_val_inputs()\n        output_data = data.get_val_outputs()\n    elif evaluation_dataset == \"test\":\n        input_data = data.get_test_inputs()\n        output_data = data.get_test_outputs()\n\n    test_input = data.input_to_torch_tensor(input_data, parameters[\"device\"], mode=\"inference\")\n    test_output = data.output_to_torch_tensor(output_data, parameters[\"device\"], mode=\"inference\")\n    test_input.requires_grad = True\n    gt_classes = test_output.max(dim=1)[1]\n\n    if parameters[\"use_batch_norm_memory\"]:\n        logits = target_network.forward(\n            test_input,\n            weights=weights,\n            condition=parameters[\"number_of_task\"],\n        )\n    else:\n        logits = target_network.forward(test_input, weights=weights)\n\n    if len(logits) == 2:\n        logits, _ = logits\n\n    predictions = logits.max(dim=1)[1]\n\n    if evaluation_dataset == \"test\":\n        # FGSM, PGD, AutoAttack, None\n\n        attack_method = \"AutoAttack\"\n        if attack_method == None:\n            pass\n            # accuracy = (\n            #                    torch.sum(gt_classes == predictions).float() / gt_classes.numel()\n            #            ) * 100.0\n            # return accuracy\n        elif attack_method == \"FGSM\":\n            ksi = 25 / 255 # attack strength\n            criterion = nn.CrossEntropyLoss()\n            loss = criterion(logits, gt_classes)\n            target_network.zero_grad()\n            loss.backward()\n\n            perturbation = torch.clamp(ksi * test_input.grad.data.sign(), -0.01,0.01)\n\n            # data_grad = test_input.grad.data\n            # signed_grad = data_grad.sign()\n            # perturbation = ksi * signed_grad\n            perturbed_test_input = test_input + perturbation\n            perturbed_test_input = torch.clamp(perturbed_test_input, 0, 1)\n\n            assert torch.max(torch.abs(perturbed_test_input - test_input)) <= 0.01\n\n            perturbed_output, _ = target_network.forward(perturbed_test_input, weights=weights)\n            perturbed_pred = perturbed_output.max(dim=1)[1]\n            perturbed_acc = (torch.sum(gt_classes == perturbed_pred).float() / gt_classes.numel()) * 100\n\n            return perturbed_acc\n        elif attack_method == 'PGD':\n            ksi = 40 / 255\n            alpha = 1\n            random_start = False\n            num_iteration = min(ksi + 4, 1.25 * ksi)\n            criterion = nn.CrossEntropyLoss()\n\n            perturbed_input = test_input.clone().detach()\n            perturbed_input.requires_grad = True\n\n            if random_start:\n                perturbed_input += torch.empty_like(perturbed_input).uniform_(-ksi, ksi)\n                perturbed_input = torch.clamp(perturbed_input, 0, 1)\n\n            for it in range(num_iteration):\n                perturbed_input.requires_grad_()\n                outputs, _ = target_network.forward(perturbed_input, weights=weights)\n\n                loss = criterion(outputs, gt_classes)\n                target_network.zero_grad()\n                loss.backward(retain_graph=True)\n\n                perturbation = alpha * perturbed_input.grad.sign()\n                perturbed_input = perturbed_input + perturbation\n                perturbed_input = torch.clamp(perturbed_input, -ksi, ksi).detach()\n\n                perturbed_input.requires_grad_()\n\n            perturbed_output, _ = target_network.forward(perturbed_input, weights=weights)\n            perturbed_pred = perturbed_output.max(dim=1)[1]\n            perturbed_acc = (torch.sum(gt_classes == perturbed_pred).float() / gt_classes.numel()) * 100.0\n\n            return perturbed_acc\n        elif attack_method == 'AutoAttack':\n            ksi = 20/255\n            adversary = AutoAttack(\n                lambda x: target_network.forward(x, weights=weights)[0],\n                norm='L2',\n                eps=ksi,\n                version='standard',\n                device=parameters[\"device\"]\n            )\n            perturbed_input = test_input.clone().detach()\n            x_adv = adversary.run_standard_evaluation(perturbed_input, gt_classes, bs=batch_size)\n\n            perturbed_output, _ = target_network.forward(x_adv, weights=weights)\n            perturbed_pred = perturbed_output.max(dim=1)[1]\n            perturbed_acc = (torch.sum(gt_classes == perturbed_pred).float() / gt_classes.numel()) * 100.0\n\n            return perturbed_acc\n\n    accuracy = torch.sum(gt_classes == predictions).float() / gt_classes.numel() * 100.0\n    return accuracy\n\n\n\ndef evaluate_previous_tasks(\n    hypernetwork,\n    target_network,\n    dataframe_results,\n    list_of_permutations,\n    sparsity_parameter,\n    parameters,\n):\n    \"\"\"\n    Evaluate the target network according to the weights generated\n    by the hypernetwork for all previously trained tasks. For instance,\n    if current_task_no is equal to 5, then tasks 0, 1, 2, 3, 4 and 5\n    will be evaluated\n\n    Arguments:\n    ----------\n      *hypernetwork* (hypnettorch.hnets module, e.g. mlp_hnet.MLP)\n                     a hypernetwork that generates weights for the target\n                     network\n      *target_network* (hypnettorch.mnets module, e.g. mlp.MLP)\n                       a target network that finally will perform\n                       classification\n      *dataframe_results* (Pandas Dataframe) stores results; contains\n                          following columns: 'after_learning_of_task',\n                          'tested_task' and 'accuracy'\n      *list_of_permutations*: (hypnettorch.data module), e.g. in the case\n                              of PermutedMNIST it will be\n                              special.permuted_mnist.PermutedMNISTList\n      *sparsity_parameter*: (float) defines which percentage of weights\n                            of the target network should be left: it will\n                            be (100-sparsity_parameter)%\n      *parameters* a dictionary containing the following keys:\n        -device- string: 'cuda' or 'cpu', defines in which device calculations\n                 will be performed\n        -use_batch_norm_memory- Boolean: defines whether stored weights\n                                of the batch normalization layer should be used\n                                If True then *number_of_task* has to be given\n        -number_of_task- int/None: gives an information which task is currently\n                         solved\n\n    Returns:\n    --------\n      *dataframe_results* (Pandas Dataframe) a dataframe updated with\n                          the calculated results\n    \"\"\"\n    # Calculate accuracy for each previously trained task\n    # as well as for the last trained task\n    # Here noise should be added to the embedding vectors.\n    hypernetwork.eval()\n    target_network.eval()\n    for task in range(parameters[\"number_of_task\"] + 1):\n        currently_tested_task = list_of_permutations[task]\n        # Generate weights of the target network\n        hypernetwork_weights = hypernetwork.forward(cond_id=task)\n        if \"weights\" in dir(target_network):\n            target_network_weights = target_network.weights\n        else:\n            target_network_weights = target_network\n\n        accuracy = calculate_accuracy(\n            currently_tested_task,\n            target_network,\n            target_network_weights,\n            parameters=parameters,\n            evaluation_dataset=\"test\",\n        )\n        result = {\n            \"after_learning_of_task\": parameters[\"number_of_task\"],\n            \"tested_task\": task,\n            \"accuracy\": accuracy.cpu().item(),\n        }\n        print(f\"Accuracy for task {task}: {accuracy}%.\")\n        dataframe_results = dataframe_results.append(result, ignore_index=True)\n    return dataframe_results\n\n\ndef save_parameters(saving_folder, parameters, name=None):\n    \"\"\"\n    Save hyperparameters to the selected file.\n\n    Arguments:\n    ----------\n      *saving_folder* (string) defines a path to the folder for saving\n      *parameters* (dictionary) contains all hyperparameters to saving\n      *name* (optional string) name of the file for saving\n    \"\"\"\n    if name is None:\n        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        name = f\"parameters_{current_time}.csv\"\n    with open(f\"{saving_folder}/{name}\", \"w\") as file:\n        for key in parameters.keys():\n            file.write(f\"{key};{parameters[key]}\\n\")\n\n\ndef plot_heatmap(load_path):\n    \"\"\"\n    Plot heatmap presenting results for different learning tasks\n\n    Argument:\n    ---------\n      *load_path* (string) contains path to the .csv file with\n                  results in a dataframe shape, i.e. with columns:\n                  'after_learning_of_task', 'tested_task' (both\n                  integers) and 'accuracy' (float)\n    \"\"\"\n    dataframe = pd.read_csv(load_path, delimiter=\";\", index_col=0)\n    dataframe = dataframe.astype(\n        {\"after_learning_of_task\": \"int32\", \"tested_task\": \"int32\"}\n    )\n    table = dataframe.pivot(\"after_learning_of_task\", \"tested_task\", \"accuracy\")\n    sns.heatmap(table, annot=True, fmt=\".1f\")\n    plt.tight_layout()\n    plt.savefig(load_path.replace(\".csv\", \".pdf\"), dpi=300)\n    plt.close()\n\ndef calculate_robustness_acc(dataframe):\n    \"\"\"\n    Calculate the ACC metric based on robustness accuracies i.e.\n    accuracies after learning last task\n\n    Argument:\n    ---------\n      *load_path* (string) contains path to the .csv file with\n                  results in a dataframe shape, i.e. with columns:\n                  'after_learning_of_task', 'tested_task' (both\n                  integers) and 'accuracy' (float).\n\n    Returns:\n    --------\n      normed_robustness_acc (float): The computed ACC metric.\n    \"\"\"\n    if len(dataframe) != 0:\n        last_task = dataframe[\"after_learning_of_task\"].max()\n        accuracies_after_last_task = dataframe[dataframe[\"after_learning_of_task\"] == last_task]\n        unnormed_robustness_acc = accuracies_after_last_task[\"accuracy\"].sum()\n        normed_robustness_acc = unnormed_robustness_acc / (last_task + 1)\n    else:\n        normed_robustness_acc = 0\n\n    return normed_robustness_acc\n\n\ndef calculate_BWT(dataframe):\n    \"\"\"\n    Calculate the BWT metric based on accuracy data.\n\n    Argument:\n    ---------\n      *load_path* (string) contains path to the .csv file with\n                  results in a dataframe shape, i.e. with columns:\n                  'after_learning_of_task', 'tested_task' (both\n                  integers) and 'accuracy' (float).\n\n    Returns:\n    --------\n      BWT (float): The computed BWT metric.\n    \"\"\"\n\n    if len(dataframe) > 1:\n        last_task = dataframe[\"after_learning_of_task\"].max()\n        accuracies_after_last_task = dataframe[dataframe[\"after_learning_of_task\"] == last_task]\n        accuracies_of_newly_learned_tasks = dataframe[\n            dataframe[\"after_learning_of_task\"] == dataframe[\"tested_task\"]\n            ]\n        robustness_sum = accuracies_after_last_task[\n            accuracies_after_last_task[\"tested_task\"] != last_task\n            ][\"accuracy\"].sum()\n        freshness_sum = accuracies_of_newly_learned_tasks[\n            accuracies_of_newly_learned_tasks[\"tested_task\"] != last_task\n            ][\"accuracy\"].sum()\n        unnormed_BWT = robustness_sum - freshness_sum\n        BWT = unnormed_BWT / last_task\n    else:\n        BWT = 0\n\n    return BWT\n\n\ndef robustness_graph(load_path):\n    \"\"\"\n    Plot graph presenting ACC metric results for different learning tasks\n\n    Argument:\n    ---------\n      *load_path* (string) contains path to the .csv file with\n                  results in a dataframe shape, i.e. with columns:\n                  'after_learning_of_task', 'tested_task' (both\n                  integers) and 'accuracy' (float)\n    \"\"\"\n    dataframe = pd.read_csv(load_path, delimiter=\";\", index_col=0)\n    dataframe = dataframe.astype(\n        {\"after_learning_of_task\": \"int32\", \"tested_task\": \"int32\"}\n    )\n    max_task = dataframe[\"after_learning_of_task\"].max()\n    partial_robustness = []\n    for task in range(max_task+1):\n        partial_dataframe = dataframe[dataframe[\"after_learning_of_task\"] <= task]\n        partial_robustness.append(calculate_robustness_acc(partial_dataframe))\n\n    sns.lineplot(x=range(max_task+1), y=partial_robustness)\n    plt.xlabel(\"Task\")\n    plt.ylabel(\"Robustness\")\n    plt.title(\"Robustness during experiment\")\n    plt.tight_layout()\n    plt.savefig(load_path.replace(\".csv\", \"_ACC.pdf\"), dpi=300)\n    plt.close()\n\n\ndef bwt_graph(load_path):\n    \"\"\"\n    Plot graph presenting BWT metric results for different learning tasks\n\n    Argument:\n    ---------\n      *load_path* (string) contains path to the .csv file with\n                  results in a dataframe shape, i.e. with columns:\n                  'after_learning_of_task', 'tested_task' (both\n                  integers) and 'accuracy' (float)\n    \"\"\"\n    dataframe = pd.read_csv(load_path, delimiter=\";\", index_col=0)\n    dataframe = dataframe.astype(\n        {\"after_learning_of_task\": \"int32\", \"tested_task\": \"int32\"}\n    )\n\n    max_task = dataframe[\"after_learning_of_task\"].max()\n    partial_bwt = []\n    for task in range(max_task):\n        partial_dataframe = dataframe[dataframe[\"after_learning_of_task\"] <= task]\n        partial_bwt.append(calculate_BWT(partial_dataframe))\n\n    sns.lineplot(x=range(max_task), y=partial_bwt)\n    plt.tight_layout()\n    plt.savefig(load_path.replace(\".csv\", \"_BWT.pdf\"), dpi=300)\n    plt.close()\n\n\ndef train_single_task(\n    hypernetwork,\n    target_network,\n    criterion,\n    parameters,\n    dataset_list_of_tasks,\n    current_no_of_task,\n):\n    \"\"\"\n    Train two neural networks: a hypernetwork will generate a sparse\n    binary mask and the weights of the target neural network are multiplied by\n    this binary mask creating a sparse network. This module operates\n    on a single training task with a specific number.\n\n    Arguments:\n    ----------\n      *hypernetwork*: (hypnettorch.hnets module, e.g. mlp_hnet.MLP)\n                      a hypernetwork that generates weights for the target\n                      network\n      *target_network*: (hypnettorch.mnets module, e.g. mlp.MLP)\n                        a target network that finally will perform\n                        classification\n      *criterion*: (torch.nn module) implements a loss function,\n                   e.g. CrossEntropyLoss\n      *parameters*: (dictionary) contains necessary hyperparameters\n                    describing an experiment\n      *dataset_list_of_tasks*: a module containing list of tasks for the CL\n                               scenario, e.g. permuted_mnist.PermutedMNISTList\n      *current_no_of_task*: (int) specifies the number of currently solving task\n\n    Returns:\n    --------\n      *hypernetwork*: a modified module of hypernetwork\n      *target_network*: a modified module of the target network\n    \"\"\"\n    if parameters[\"optimizer\"] == \"adam\":\n        optimizer = torch.optim.Adam(\n            [*hypernetwork.parameters()],\n            lr=parameters[\"learning_rate\"],\n        )\n    elif parameters[\"optimizer\"] == \"rmsprop\":\n        optimizer = torch.optim.RMSprop(\n            [*hypernetwork.parameters()],\n            lr=parameters[\"learning_rate\"],\n        )\n    else:\n        raise ValueError(\"Wrong type of the selected optimizer!\")\n    if parameters[\"best_model_selection_method\"] == \"val_loss\":\n        # Store temporary best models to keep those with the highest\n        # validation accuracy.\n        best_hypernetwork = deepcopy(hypernetwork)\n        best_target_network = deepcopy(target_network)\n        best_val_accuracy = 0.0\n    elif parameters[\"best_model_selection_method\"] != \"last_model\":\n        raise ValueError(\n            \"Wrong value of best_model_selection_method parameter!\"\n        )\n    # Compute targets for the regularization part of loss before starting\n    # the training of a current task\n    hypernetwork.train()\n    print(f\"task: {current_no_of_task}\")\n    if current_no_of_task > 0:\n        regularization_targets = hreg.get_current_targets(\n            current_no_of_task, hypernetwork\n        )\n        previous_hnet_theta = None\n        previous_hnet_embeddings = None\n\n    if (parameters[\"target_network\"] == \"ResNet\") and parameters[\"use_batch_norm\"]:\n        use_batch_norm_memory = True\n    else:\n        use_batch_norm_memory = False\n    current_dataset_instance = dataset_list_of_tasks[current_no_of_task]\n    # If training through a given number of epochs is desired\n    # the number of iterations has to be calculated\n    if parameters[\"number_of_epochs\"] is not None:\n        (\n            no_of_iterations_per_epoch,\n            parameters[\"number_of_iterations\"],\n        ) = calculate_number_of_iterations(\n            current_dataset_instance.num_train_samples,\n            parameters[\"batch_size\"],\n            parameters[\"number_of_epochs\"],\n        )\n        # Scheduler can be set only when the number of epochs is given\n        if parameters[\"lr_scheduler\"]:\n            current_epoch = 0\n            plateau_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer,\n                \"max\",\n                factor=np.sqrt(0.1),\n                patience=5,\n                min_lr=0.5e-6,\n                cooldown=0,\n                verbose=True,\n            )\n    for iteration in range(parameters[\"number_of_iterations\"]):\n\n        current_batch = current_dataset_instance.next_train_batch(parameters[\"batch_size\"])\n        tensor_input = current_dataset_instance.input_to_torch_tensor(\n            current_batch[0], parameters[\"device\"], mode=\"train\"\n        )\n        tensor_output = current_dataset_instance.output_to_torch_tensor(\n            current_batch[1], parameters[\"device\"], mode=\"train\"\n        )\n        gt_output = tensor_output.max(dim=1)[1]\n        optimizer.zero_grad()\n        loss_norm_target_regularizer = 0.0\n\n        if \"weights\" in dir(target_network):\n            target_weights = target_network.weights\n        else:\n            target_weights = target_network\n\n        if parameters[\"target_network\"] == \"epsMLP\":\n            default_eps = target_network.epsilon\n            total_iterations = parameters[\"number_of_iterations\"]\n            inv_total_iterations = 1 / total_iterations\n            target_network.epsilon = iteration * inv_total_iterations * default_eps\n\n            prediction, eps_prediction = target_network.forward(\n                tensor_input, weights=target_weights\n            )\n\n            z_lower = prediction - eps_prediction.T\n            z_upper = prediction + eps_prediction.T\n            z = torch.where((nn.functional.one_hot(gt_output, prediction.size(-1))).bool(), z_lower, z_upper)\n\n            loss_spec = criterion(z, gt_output)\n            loss_fit = criterion(prediction, gt_output)\n            kappa = 1 - (iteration * inv_total_iterations * 0.5)\n\n            loss_current_task = kappa * loss_fit + (1 - kappa) * loss_spec\n            target_network.epsilon = default_eps\n        else:\n            prediction = target_network.forward(tensor_input, weights=target_weights)\n            loss_current_task = criterion(prediction, gt_output)\n            print(f' loss: {loss_current_task.item()}')\n        loss_regularization = 0.0\n        if current_no_of_task > 0:\n            loss_regularization = hreg.calc_fix_target_reg(\n                hypernetwork,\n                current_no_of_task,\n                targets=regularization_targets,\n                mnet=target_network,\n                prev_theta=previous_hnet_theta,\n                prev_task_embs=previous_hnet_embeddings,\n                inds_of_out_heads=None,\n                batch_size=-1,\n            )\n        append_row_to_file(\n            f'{parameters[\"saving_folder\"]}regularization_loss.csv',\n            f\"{current_no_of_task};{iteration};\"\n            f\"{loss_regularization};{loss_norm_target_regularizer}\",\n        )\n        loss = (\n            loss_current_task\n            + parameters[\"beta\"]\n            * loss_regularization\n            / max(1, current_no_of_task)\n            + parameters[\"lambda\"] * loss_norm_target_regularizer\n        )\n\n        loss.backward()\n        optimizer.step()\n\n        if parameters[\"number_of_epochs\"] is None:\n            condition = (iteration % 100 == 0) or (iteration == (parameters[\"number_of_iterations\"] - 1))\n        else:\n            condition = (\n                (iteration % 100 == 0)\n                or (iteration == (parameters[\"number_of_iterations\"] - 1))\n                or (((iteration + 1) % no_of_iterations_per_epoch) == 0)\n            )\n\n        if condition:\n            if parameters[\"number_of_epochs\"] is not None:\n                current_epoch = (iteration + 1) // no_of_iterations_per_epoch\n                print(f\"Current epoch: {current_epoch}\")\n            accuracy = calculate_accuracy(\n                current_dataset_instance,\n                target_network,\n                target_weights,\n                parameters={\n                    \"device\": parameters[\"device\"],\n                    \"use_batch_norm_memory\": use_batch_norm_memory,\n                    \"number_of_task\": current_no_of_task,\n                },\n                evaluation_dataset=\"validation\",\n            )\n            print(\n                f\"Task {current_no_of_task}, iteration: {iteration + 1},\"\n                f\" loss: {loss.item()}, validation accuracy: {accuracy}\"\n            )\n            # If the accuracy on the validation dataset is higher\n            # than previously\n            if parameters[\"best_model_selection_method\"] == \"val_loss\":\n                if accuracy > best_val_accuracy:\n                    best_val_accuracy = accuracy\n                    best_hypernetwork = deepcopy(hypernetwork)\n                    best_target_network = deepcopy(target_network)\n            if (\n                parameters[\"number_of_epochs\"] is not None\n                and parameters[\"lr_scheduler\"]\n                and (((iteration + 1) % no_of_iterations_per_epoch) == 0)\n            ):\n                print(\"Finishing the current epoch\")\n                plateau_scheduler.step(accuracy)\n\n    if parameters[\"best_model_selection_method\"] == \"val_loss\":\n        return best_hypernetwork, best_target_network\n    else:\n        return hypernetwork, target_network\n\ndef build_multiple_task_experiment(\n    dataset_list_of_tasks, parameters, use_chunks=False\n):\n    \"\"\"\n    Create a continual learning experiment with multiple tasks\n    for a given dataset.\n\n    Arguments:\n    ----------\n      *dataset_list_of_tasks*: a module containing list of tasks for the CL\n                               scenario, e.g. permuted_mnist.PermutedMNISTList\n      *parameters*: (dictionary) contains necessary hyperparameters\n                    describing an experiment\n      *use_chunks*: (Boolean value) optional argument, defines whether\n                    a hypernetwork should generate weights in chunks or not\n\n    Returns:\n    --------\n      *hypernetwork*: (hypnettorch.hnets module, e.g. mlp_hnet.MLP)\n                      a hypernetwork that generates weights for the target\n                      network\n      *target_network*: (hypnettorch.mnets module, e.g. mlp.MLP)\n                        a target network that finally will perform\n                        classification\n      *dataframe*: (Pandas Dataframe) contains results from consecutive\n                   evaluations for all previous tasks\n    \"\"\"\n    output_shape = list(dataset_list_of_tasks[0].get_train_outputs())[0].shape[0]\n    if parameters[\"target_network\"] == \"MLP\":\n        target_network = MLP(\n            n_in=parameters[\"input_shape\"],\n            n_out=output_shape,\n            hidden_layers=parameters[\"target_hidden_layers\"],\n            use_bias=parameters[\"use_bias\"],\n            no_weights=True,\n        ).to(parameters[\"device\"])\n    elif parameters[\"target_network\"] == \"epsMLP\":\n        target_network = epsMLP(\n            n_in=parameters[\"input_shape\"],\n            n_out=output_shape,\n            hidden_layers=parameters[\"target_hidden_layers\"],\n            use_bias=parameters[\"use_bias\"],\n            no_weights=True,\n            epsilon=0.01,\n        ).to(parameters[\"device\"])\n    elif parameters[\"target_network\"] == \"ResNet\":\n        target_network = ResNet(\n            in_shape=(parameters[\"input_shape\"], parameters[\"input_shape\"], 3),\n            use_bias=parameters[\"use_bias\"],\n            num_classes=output_shape,\n            n=parameters[\"resnet_number_of_layer_groups\"],\n            k=parameters[\"resnet_widening_factor\"],\n            no_weights=True,\n            use_batch_norm=parameters[\"use_batch_norm\"],\n            bn_track_stats=False,\n        ).to(parameters[\"device\"])\n    elif parameters[\"target_network\"] == \"ZenkeNet\":\n        if parameters[\"dataset\"] in [\"CIFAR100\", \"CIFAR100_FeCAM_setup\"]:\n            architecture = \"cifar\"\n        elif parameters[\"dataset\"] == \"TinyImageNet\":\n            architecture = \"tiny\"\n        else:\n            raise ValueError(\"This dataset is currently not implemented!\")\n        target_network = ZenkeNet(\n            in_shape=(parameters[\"input_shape\"], parameters[\"input_shape\"], 3),\n            num_classes=output_shape,\n            arch=architecture,\n            no_weights=True,\n        ).to(parameters[\"device\"])\n    # Create a hypernetwork based on the shape of the target network\n    if not use_chunks:\n        hypernetwork = HMLP(\n            target_network.param_shapes[0:],\n            uncond_in_size=0,\n            cond_in_size=parameters[\"embedding_size\"],\n            activation_fn=parameters[\"activation_function\"],\n            layers=parameters[\"hypernetwork_hidden_layers\"],\n            num_cond_embs=parameters[\"number_of_tasks\"],\n        ).to(parameters[\"device\"])\n    else:\n        hypernetwork = ChunkedHMLP(\n            target_shapes=target_network.param_shapes[0:],\n            chunk_size=parameters[\"chunk_size\"],\n            chunk_emb_size=parameters[\"chunk_emb_size\"],\n            cond_in_size=parameters[\"embedding_size\"],\n            activation_fn=parameters[\"activation_function\"],\n            layers=parameters[\"hypernetwork_hidden_layers\"],\n            num_cond_embs=parameters[\"number_of_tasks\"],\n        ).to(parameters[\"device\"])\n\n    criterion = nn.CrossEntropyLoss()\n    dataframe = pd.DataFrame(columns=[\"after_learning_of_task\", \"tested_task\", \"accuracy\"])\n\n    if (parameters[\"target_network\"] == \"ResNet\") and parameters[\"use_batch_norm\"]:\n        use_batch_norm_memory = True\n    else:\n        use_batch_norm_memory = False\n    hypernetwork.train()\n    target_network.train()\n    for no_of_task in range(parameters[\"number_of_tasks\"]):\n        target_network = torch.load(\"Results/grid_search/permuted_mnist/0/target_network_after_9_task.pt\")\n        hypernetwork = torch.load(\"Results/grid_search/permuted_mnist/0/hypernetwork_after_9_task.pt\")\n\n        if no_of_task <= (parameters[\"number_of_tasks\"] - 1):\n            # Save current state of networks\n            write_pickle_file(\n                f'{parameters[\"saving_folder\"]}/'\n                f\"hypernetwork_after_{no_of_task}_task\",\n                hypernetwork.weights,\n            )\n            write_pickle_file(\n                f'{parameters[\"saving_folder\"]}/'\n                f\"target_network_after_{no_of_task}_task\",\n                target_network.weights,\n            )\n        dataframe = evaluate_previous_tasks(\n            hypernetwork,\n            target_network,\n            dataframe,\n            dataset_list_of_tasks,\n            parameters[\"sparsity_parameter\"],\n            parameters={\n                \"device\": parameters[\"device\"],\n                \"use_batch_norm_memory\": use_batch_norm_memory,\n                \"number_of_task\": no_of_task,\n            },\n        )\n        dataframe = dataframe.astype(\n            {\"after_learning_of_task\": \"int\", \"tested_task\": \"int\"}\n        )\n\n        print(f\"Robustness accuracy: {calculate_robustness_acc(dataframe)}\")\n        print(f\"BWT accuracy: {calculate_BWT(dataframe)}\")\n\n        dataframe.to_csv(\n            f'{parameters[\"saving_folder\"]}/'\n            f'results_{parameters[\"name_suffix\"]}.csv',\n            sep=\";\",\n        )\n\n    return hypernetwork, target_network, dataframe\n\n\ndef main_running_experiments(path_to_datasets, parameters):\n    \"\"\"\n    Perform a series of experiments based on the hyperparameters.\n\n    Arguments:\n    ----------\n      *path_to_datasets*: (str) path to files with datasets\n      *parameters*: (dict) contains multiple experiment hyperparameters\n\n    Returns learned hypernetwork, target network and a dataframe\n    with single results.\n    \"\"\"\n    if parameters[\"dataset\"] == \"PermutedMNIST\":\n        dataset_tasks_list = prepare_permuted_mnist_tasks(\n            path_to_datasets,\n            parameters[\"input_shape\"],\n            parameters[\"number_of_tasks\"],\n            parameters[\"padding\"],\n            parameters[\"no_of_validation_samples\"],\n        )\n    elif parameters[\"dataset\"] == \"CIFAR100\":\n        dataset_tasks_list = prepare_split_cifar100_tasks(\n            path_to_datasets,\n            validation_size=parameters[\"no_of_validation_samples\"],\n            use_augmentation=parameters[\"augmentation\"],\n        )\n    elif parameters[\"dataset\"] == \"SplitMNIST\":\n        dataset_tasks_list = prepare_split_mnist_tasks(\n            path_to_datasets,\n            validation_size=parameters[\"no_of_validation_samples\"],\n            use_augmentation=parameters[\"augmentation\"],\n            number_of_tasks=parameters[\"number_of_tasks\"],\n        )\n    elif parameters[\"dataset\"] == \"TinyImageNet\":\n        dataset_tasks_list = prepare_tinyimagenet_tasks(\n            path_to_datasets,\n            seed=parameters[\"seed\"],\n            validation_size=parameters[\"no_of_validation_samples\"],\n            number_of_tasks=parameters[\"number_of_tasks\"],\n        )\n    elif parameters[\"dataset\"] == \"CIFAR100_FeCAM_setup\":\n        dataset_tasks_list = prepare_split_cifar100_tasks_aka_FeCAM(\n            path_to_datasets,\n            number_of_tasks=parameters[\"number_of_tasks\"],\n            no_of_validation_samples_per_class=parameters[\n                \"no_of_validation_samples_per_class\"\n            ],\n            use_augmentation=parameters[\"augmentation\"],\n        )\n    else:\n        raise ValueError(\"Wrong name of the dataset!\")\n\n    hypernetwork, target_network, dataframe = build_multiple_task_experiment(\n        dataset_tasks_list, parameters, use_chunks=parameters[\"use_chunks\"]\n    )\n    # Calculate statistics of grid search results\n    no_of_last_task = parameters[\"number_of_tasks\"] - 1\n    accuracies = dataframe.loc[\n        dataframe[\"after_learning_of_task\"] == no_of_last_task\n    ][\"accuracy\"].values\n    row_with_results = (\n        f\"{dataset_tasks_list[0].get_identifier()};\"\n        f'{parameters[\"augmentation\"]};'\n        f'{parameters[\"embedding_size\"]};'\n        f'{parameters[\"seed\"]};'\n        f'{str(parameters[\"hypernetwork_hidden_layers\"]).replace(\" \", \"\")};'\n        f'{parameters[\"use_chunks\"]};{parameters[\"chunk_emb_size\"]};'\n        f'{parameters[\"target_network\"]};'\n        f'{str(parameters[\"target_hidden_layers\"]).replace(\" \", \"\")};'\n        f'{parameters[\"resnet_number_of_layer_groups\"]};'\n        f'{parameters[\"resnet_widening_factor\"]};'\n        f'{parameters[\"norm_regularizer_masking\"]};'\n        f'{parameters[\"best_model_selection_method\"]};'\n        f'{parameters[\"optimizer\"]};'\n        f'{parameters[\"activation_function\"]};'\n        f'{parameters[\"learning_rate\"]};{parameters[\"batch_size\"]};'\n        f'{parameters[\"beta\"]};{parameters[\"sparsity_parameter\"]};'\n        f'{parameters[\"norm\"]};{parameters[\"lambda\"]};'\n        f\"{np.mean(accuracies)};{np.std(accuracies)}\"\n    )\n    append_row_to_file(\n        f'{parameters[\"grid_search_folder\"]}'\n        f'{parameters[\"summary_results_filename\"]}.csv',\n        row_with_results,\n    )\n\n    load_path = (\n        f'{parameters[\"saving_folder\"]}/'\n        f'results_{parameters[\"name_suffix\"]}.csv'\n    )\n    plot_heatmap(load_path)\n    robustness_graph(load_path)\n    bwt_graph(load_path)\n\n    return hypernetwork, target_network, dataframe\n\n\nif __name__ == \"__main__\":\n    path_to_datasets = \"./Data\"\n    dataset = \"PermutedMNIST\"\n    # 'PermutedMNIST', 'CIFAR100', 'SplitMNIST', 'TinyImageNet',\n    # 'CIFAR100_FeCAM_setup'\n    part = 1\n    create_grid_search = True\n    if create_grid_search:\n        summary_results_filename = \"grid_search_results\"\n    else:\n        summary_results_filename = \"summary_results\"\n    hyperparameters = set_hyperparameters(\n        dataset, grid_search=create_grid_search, part=part\n    )\n    header = (\n        \"dataset_name;augmentation;embedding_size;seed;hypernetwork_hidden_layers;\"\n        \"use_chunks;chunk_emb_size;target_network;target_hidden_layers;\"\n        \"layer_groups;widening;norm_regularizer_masking;final_model;optimizer;\"\n        \"hypernet_activation_function;learning_rate;batch_size;beta;\"\n        \"sparsity;norm;lambda;mean_accuracy;std_accuracy\"\n    )\n    append_row_to_file(\n        f'{hyperparameters[\"saving_folder\"]}{summary_results_filename}.csv',\n        header,\n    )\n\n    for no, elements in enumerate(\n        product(\n            hyperparameters[\"embedding_sizes\"],\n            hyperparameters[\"learning_rates\"],\n            hyperparameters[\"betas\"],\n            hyperparameters[\"hypernetworks_hidden_layers\"],\n            hyperparameters[\"sparsity_parameters\"],\n            hyperparameters[\"lambdas\"],\n            hyperparameters[\"batch_sizes\"],\n            hyperparameters[\"norm_regularizer_masking_opts\"],\n            hyperparameters[\"seed\"],\n        )\n    ):\n        embedding_size = elements[0]\n        learning_rate = elements[1]\n        beta = elements[2]\n        hypernetwork_hidden_layers = elements[3]\n        sparsity_parameter = elements[4]\n        lambda_par = elements[5]\n        batch_size = elements[6]\n        norm_regularizer_masking = elements[7]\n        # Of course, seed is not optimized, but it is easier to prepare experiments for multiple seeds in such a way\n        seed = elements[8]\n\n        parameters = {\n            \"input_shape\": hyperparameters[\"shape\"],\n            \"augmentation\": hyperparameters[\"augmentation\"],\n            \"number_of_tasks\": hyperparameters[\"number_of_tasks\"],\n            \"dataset\": dataset,\n            \"seed\": seed,\n            \"hypernetwork_hidden_layers\": hypernetwork_hidden_layers,\n            \"activation_function\": hyperparameters[\"activation_function\"],\n            \"norm_regularizer_masking\": norm_regularizer_masking,\n            \"use_chunks\": hyperparameters[\"use_chunks\"],\n            \"chunk_size\": hyperparameters[\"chunk_size\"],\n            \"chunk_emb_size\": hyperparameters[\"chunk_emb_size\"],\n            \"target_network\": hyperparameters[\"target_network\"],\n            \"target_hidden_layers\": hyperparameters[\"target_hidden_layers\"],\n            \"resnet_number_of_layer_groups\": hyperparameters[\n                \"resnet_number_of_layer_groups\"\n            ],\n            \"resnet_widening_factor\": hyperparameters[\"resnet_widening_factor\"],\n            \"adaptive_sparsity\": hyperparameters[\"adaptive_sparsity\"],\n            \"sparsity_parameter\": sparsity_parameter,\n            \"learning_rate\": learning_rate,\n            \"best_model_selection_method\": hyperparameters[\n                \"best_model_selection_method\"\n            ],\n            \"lr_scheduler\": hyperparameters[\"lr_scheduler\"],\n            \"batch_size\": batch_size,\n            \"number_of_epochs\": hyperparameters[\"number_of_epochs\"],\n            \"number_of_iterations\": hyperparameters[\"number_of_iterations\"],\n            \"no_of_validation_samples_per_class\": hyperparameters[\n                \"no_of_validation_samples_per_class\"\n            ],\n            \"embedding_size\": embedding_size,\n            \"norm\": hyperparameters[\"norm\"],\n            \"lambda\": lambda_par,\n            \"optimizer\": hyperparameters[\"optimizer\"],\n            \"beta\": beta,\n            \"padding\": hyperparameters[\"padding\"],\n            \"use_bias\": hyperparameters[\"use_bias\"],\n            \"use_batch_norm\": hyperparameters[\"use_batch_norm\"],\n            \"device\": hyperparameters[\"device\"],\n            \"saving_folder\": f'{hyperparameters[\"saving_folder\"]}{no}/',\n            \"grid_search_folder\": hyperparameters[\"saving_folder\"],\n            \"save_masks\": hyperparameters[\"save_consecutive_masks\"],\n            \"name_suffix\": f\"mask_sparsity_{sparsity_parameter}\",\n            \"summary_results_filename\": summary_results_filename,\n        }\n        if \"no_of_validation_samples\" in hyperparameters:\n            parameters[\"no_of_validation_samples\"] = hyperparameters[\n                \"no_of_validation_samples\"\n            ]\n\n        os.makedirs(parameters[\"saving_folder\"], exist_ok=True)\n        # start_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        save_parameters(\n            parameters[\"saving_folder\"],\n            parameters,\n            name=f'parameters_{parameters[\"name_suffix\"]}.csv',\n        )\n\n        # Important! Seed is set before the preparation of the dataset!\n        if seed is not None:\n            set_seed(seed)\n\n        hypernetwork, target_network, dataframe = main_running_experiments(\n            path_to_datasets, parameters\n        )\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	(revision 9bcf188f17f73fe837356539a1ef3df5f2cd5485)
+++ b/main.py	(date 1739554589439)
@@ -815,12 +815,6 @@
         hypernetwork = torch.load("Results/grid_search/permuted_mnist/0/hypernetwork_after_9_task.pt")
 
         if no_of_task <= (parameters["number_of_tasks"] - 1):
-            # Save current state of networks
-            write_pickle_file(
-                f'{parameters["saving_folder"]}/'
-                f"hypernetwork_after_{no_of_task}_task",
-                hypernetwork.weights,
-            )
             write_pickle_file(
                 f'{parameters["saving_folder"]}/'
                 f"target_network_after_{no_of_task}_task",
@@ -953,7 +947,7 @@
 
 if __name__ == "__main__":
     path_to_datasets = "./Data"
-    dataset = "PermutedMNIST"
+    dataset = "SplitMNIST"
     # 'PermutedMNIST', 'CIFAR100', 'SplitMNIST', 'TinyImageNet',
     # 'CIFAR100_FeCAM_setup'
     part = 1
