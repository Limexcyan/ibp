Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport random\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch.optim as optim\nfrom hypnettorch.mnets import MLP\nfrom hypnettorch.mnets.resnet import ResNet\nfrom epsMLP import epsMLP\n\nfrom ZenkeNet64 import ZenkeNet\nfrom hypnettorch.hnets import HMLP\nfrom hypnettorch.hnets.chunked_mlp_hnet import ChunkedHMLP\nimport hypnettorch.utils.hnet_regularizer as hreg\nfrom torch import nn\nfrom datetime import datetime\nfrom itertools import product\nfrom copy import deepcopy\nfrom retry import retry\n\nfrom torchattacks import FGSM, PGD, AutoAttack\n\nfrom datasets import (\n    set_hyperparameters,\n    prepare_split_cifar100_tasks,\n    prepare_split_cifar100_tasks_aka_FeCAM,\n    prepare_permuted_mnist_tasks,\n    prepare_split_mnist_tasks,\n    prepare_tinyimagenet_tasks,\n)\n\nfrom autoattack import AutoAttack\n\ndef set_seed(value):\n    \"\"\"\n    Set deterministic results according to the given value\n    (including random, numpy and torch libraries)\n    \"\"\"\n    random.seed(value)\n    np.random.seed(value)\n    torch.manual_seed(value)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef append_row_to_file(filename, elements):\n    \"\"\"\n    Append a single row to the given file.\n\n    Parameters\n    ----------\n    filename: folder and name of file\n    elements: elements to saving in filename\n    \"\"\"\n    if not filename.endswith(\".csv\"):\n        filename += \".csv\"\n    filename = filename.replace(\".pt\", \"\")\n    with open(filename, \"a+\") as stream:\n        np.savetxt(\n            stream, np.array(elements)[np.newaxis], delimiter=\";\", fmt=\"%s\"\n        )\n\n\ndef write_pickle_file(filename, object_to_save):\n    torch.save(object_to_save, f\"{filename}.pt\")\n\n\n@retry((OSError, IOError))\ndef load_pickle_file(filename):\n    return torch.load(filename)\n\n\ndef get_shapes_of_network(model):\n    \"\"\"\n    Get shape of all layers of the loaded model.\n\n    Argument:\n    ---------\n      *model*: an instance of hypnettorch model, e.g. MLP from mnets\n\n    Returns:\n    --------\n      A list with lists of shapes of consecutive network layers\n    \"\"\"\n    shapes_of_model = []\n    for layer in model.weights:\n        shapes_of_model.append(list(layer.shape))\n    return shapes_of_model\n\n\ndef calculate_number_of_iterations(\n    number_of_samples, batch_size, number_of_epochs\n):\n    \"\"\"\n    Calculate the total number of iterations based on the number\n    of samples, desired batch size and number of training epochs.\n\n    Arguments:\n    ----------\n      *number_of_samples* (int) a number of individual samples\n      *batch_size* (int) a number of samples entering the network\n                   at one iteration\n      *number_of_epochs* (int) a desired number of training epochs\n\n    Returns:\n    --------\n      *no_of_iterations_per_epoch* (int) a number of training iterations\n                                   per one epoch\n      *total_no_of_iterations* (int) a total number of training iterations\n    \"\"\"\n    no_of_iterations_per_epoch = int(np.ceil(number_of_samples / batch_size))\n    total_no_of_iterations = int(no_of_iterations_per_epoch * number_of_epochs)\n    return no_of_iterations_per_epoch, total_no_of_iterations\n\ndef calculate_accuracy(data, target_network, weights, parameters, evaluation_dataset):\n    \"\"\"\n    Calculate accuracy for a given dataset using a selected network\n    and a selected set of weights. Optionally applies FGSM or PGD attack.\n\n    Arguments:\n    ----------\n      *data*: Dataset instance (e.g., hypnettorch.data.special.permuted_mnist.PermutedMNIST)\n              in the case of the PermutedMNIST dataset.\n      *target_network*: An instance of the network to be evaluated.\n      *weights*: Weights for the *target_network* network\n                 (an instance of torch.nn.modules.container.ParameterList).\n      *parameters*: Dictionary containing the following keys:\n        - 'device': (string) 'cuda' or 'cpu', determines the computation device.\n        - 'use_batch_norm_memory': (boolean) Whether to use stored weights for batch\n                                   normalization layers. If True, 'number_of_task'\n                                   must also be provided.\n        - 'number_of_task': (int or None) The task index currently being solved.\n                            Required if 'use_batch_norm_memory' is True.\n      *evaluation_dataset*: (string) Either 'validation' or 'test', specifies the dataset\n                            to evaluate.\n\n    Returns:\n    --------\n      torch.Tensor: Accuracy (percentage) for the selected setting.\n    \"\"\"\n    assert (\n        parameters[\"use_batch_norm_memory\"]\n        and parameters[\"number_of_task\"]is not None\n    ) or not parameters[\"use_batch_norm_memory\"]\n    assert evaluation_dataset in [\"validation\", \"test\"]\n    target_network.eval()\n    torch.no_grad()\n    if evaluation_dataset == \"validation\":\n        input_data = data.get_val_inputs()\n        output_data = data.get_val_outputs()\n    elif evaluation_dataset == \"test\":\n        input_data = data.get_test_inputs()\n        output_data = data.get_test_outputs()\n\n    test_input = data.input_to_torch_tensor(input_data, parameters[\"device\"], mode=\"inference\")\n    test_output = data.output_to_torch_tensor(output_data, parameters[\"device\"], mode=\"inference\")\n    test_input.requires_grad = True\n    gt_classes = test_output.max(dim=1)[1]\n\n    if parameters[\"use_batch_norm_memory\"]:\n        logits = target_network.forward(\n            test_input,\n            weights=weights,\n            condition=parameters[\"number_of_task\"],\n        )\n    else:\n        logits = target_network.forward(test_input, weights=weights)\n\n    if len(logits) == 2:\n        logits, _ = logits\n\n    predictions = logits.max(dim=1)[1]\n\n    if evaluation_dataset == \"test\":\n        # FGSM, PGD, AutoAttack, None\n\n        attack_method = \"AutoAttack\"\n        if attack_method == None:\n            pass\n            # accuracy = (\n            #                    torch.sum(gt_classes == predictions).float() / gt_classes.numel()\n            #            ) * 100.0\n            # return accuracy\n        elif attack_method == \"FGSM\":\n            ksi = 25 / 255 # attack strength\n            criterion = nn.CrossEntropyLoss()\n            loss = criterion(logits, gt_classes)\n            target_network.zero_grad()\n            loss.backward()\n\n            perturbation = torch.clamp(ksi * test_input.grad.data.sign(), -0.01,0.01)\n\n            # data_grad = test_input.grad.data\n            # signed_grad = data_grad.sign()\n            # perturbation = ksi * signed_grad\n            perturbed_test_input = test_input + perturbation\n            perturbed_test_input = torch.clamp(perturbed_test_input, 0, 1)\n\n            assert torch.max(torch.abs(perturbed_test_input - test_input)) <= 0.01\n\n            perturbed_output, _ = target_network.forward(perturbed_test_input, weights=weights)\n            perturbed_pred = perturbed_output.max(dim=1)[1]\n            perturbed_acc = (torch.sum(gt_classes == perturbed_pred).float() / gt_classes.numel()) * 100\n\n            return perturbed_acc\n        elif attack_method == 'PGD':\n            ksi = 40 / 255\n            alpha = 1\n            random_start = False\n            num_iteration = min(ksi + 4, 1.25 * ksi)\n            criterion = nn.CrossEntropyLoss()\n\n            perturbed_input = test_input.clone().detach()\n            perturbed_input.requires_grad = True\n\n            if random_start:\n                perturbed_input += torch.empty_like(perturbed_input).uniform_(-ksi, ksi)\n                perturbed_input = torch.clamp(perturbed_input, 0, 1)\n\n            for it in range(num_iteration):\n                perturbed_input.requires_grad_()\n                outputs, _ = target_network.forward(perturbed_input, weights=weights)\n\n                loss = criterion(outputs, gt_classes)\n                target_network.zero_grad()\n                loss.backward(retain_graph=True)\n\n                perturbation = alpha * perturbed_input.grad.sign()\n                perturbed_input = perturbed_input + perturbation\n                perturbed_input = torch.clamp(perturbed_input, -ksi, ksi).detach()\n\n                perturbed_input.requires_grad_()\n\n            perturbed_output, _ = target_network.forward(perturbed_input, weights=weights)\n            perturbed_pred = perturbed_output.max(dim=1)[1]\n            perturbed_acc = (torch.sum(gt_classes == perturbed_pred).float() / gt_classes.numel()) * 100.0\n\n            return perturbed_acc\n        elif attack_method == 'AutoAttack':\n            ksi = 20/255\n            adversary = AutoAttack(\n                lambda x: target_network.forward(x, weights=weights)[0],\n                norm='L2',\n                eps=ksi,\n                version='standard',\n                device=parameters[\"device\"]\n            )\n            perturbed_input = test_input.clone().detach()\n            x_adv = adversary.run_standard_evaluation(perturbed_input, gt_classes, bs=batch_size)\n\n            perturbed_output, _ = target_network.forward(x_adv, weights=weights)\n            perturbed_pred = perturbed_output.max(dim=1)[1]\n            perturbed_acc = (torch.sum(gt_classes == perturbed_pred).float() / gt_classes.numel()) * 100.0\n\n            return perturbed_acc\n\n    accuracy = torch.sum(gt_classes == predictions).float() / gt_classes.numel() * 100.0\n    return accuracy\n\n\n\ndef evaluate_previous_tasks(\n    hypernetwork,\n    target_network,\n    dataframe_results,\n    list_of_permutations,\n    sparsity_parameter,\n    parameters,\n):\n    \"\"\"\n    Evaluate the target network according to the weights generated\n    by the hypernetwork for all previously trained tasks. For instance,\n    if current_task_no is equal to 5, then tasks 0, 1, 2, 3, 4 and 5\n    will be evaluated\n\n    Arguments:\n    ----------\n      *hypernetwork* (hypnettorch.hnets module, e.g. mlp_hnet.MLP)\n                     a hypernetwork that generates weights for the target\n                     network\n      *target_network* (hypnettorch.mnets module, e.g. mlp.MLP)\n                       a target network that finally will perform\n                       classification\n      *dataframe_results* (Pandas Dataframe) stores results; contains\n                          following columns: 'after_learning_of_task',\n                          'tested_task' and 'accuracy'\n      *list_of_permutations*: (hypnettorch.data module), e.g. in the case\n                              of PermutedMNIST it will be\n                              special.permuted_mnist.PermutedMNISTList\n      *sparsity_parameter*: (float) defines which percentage of weights\n                            of the target network should be left: it will\n                            be (100-sparsity_parameter)%\n      *parameters* a dictionary containing the following keys:\n        -device- string: 'cuda' or 'cpu', defines in which device calculations\n                 will be performed\n        -use_batch_norm_memory- Boolean: defines whether stored weights\n                                of the batch normalization layer should be used\n                                If True then *number_of_task* has to be given\n        -number_of_task- int/None: gives an information which task is currently\n                         solved\n\n    Returns:\n    --------\n      *dataframe_results* (Pandas Dataframe) a dataframe updated with\n                          the calculated results\n    \"\"\"\n    # Calculate accuracy for each previously trained task\n    # as well as for the last trained task\n    # Here noise should be added to the embedding vectors.\n    hypernetwork.eval()\n    target_network.eval()\n    for task in range(parameters[\"number_of_task\"] + 1):\n        currently_tested_task = list_of_permutations[task]\n        # Generate weights of the target network\n        hypernetwork_weights = hypernetwork.forward(cond_id=task)\n        if \"weights\" in dir(target_network):\n            target_network_weights = target_network.weights\n        else:\n            target_network_weights = target_network\n\n        accuracy = calculate_accuracy(\n            currently_tested_task,\n            target_network,\n            target_network_weights,\n            parameters=parameters,\n            evaluation_dataset=\"test\",\n        )\n        result = {\n            \"after_learning_of_task\": parameters[\"number_of_task\"],\n            \"tested_task\": task,\n            \"accuracy\": accuracy.cpu().item(),\n        }\n        print(f\"Accuracy for task {task}: {accuracy}%.\")\n        dataframe_results = dataframe_results.append(result, ignore_index=True)\n    return dataframe_results\n\n\ndef save_parameters(saving_folder, parameters, name=None):\n    \"\"\"\n    Save hyperparameters to the selected file.\n\n    Arguments:\n    ----------\n      *saving_folder* (string) defines a path to the folder for saving\n      *parameters* (dictionary) contains all hyperparameters to saving\n      *name* (optional string) name of the file for saving\n    \"\"\"\n    if name is None:\n        current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        name = f\"parameters_{current_time}.csv\"\n    with open(f\"{saving_folder}/{name}\", \"w\") as file:\n        for key in parameters.keys():\n            file.write(f\"{key};{parameters[key]}\\n\")\n\n\ndef plot_heatmap(load_path):\n    \"\"\"\n    Plot heatmap presenting results for different learning tasks\n\n    Argument:\n    ---------\n      *load_path* (string) contains path to the .csv file with\n                  results in a dataframe shape, i.e. with columns:\n                  'after_learning_of_task', 'tested_task' (both\n                  integers) and 'accuracy' (float)\n    \"\"\"\n    dataframe = pd.read_csv(load_path, delimiter=\";\", index_col=0)\n    dataframe = dataframe.astype(\n        {\"after_learning_of_task\": \"int32\", \"tested_task\": \"int32\"}\n    )\n    table = dataframe.pivot(\"after_learning_of_task\", \"tested_task\", \"accuracy\")\n    sns.heatmap(table, annot=True, fmt=\".1f\")\n    plt.tight_layout()\n    plt.savefig(load_path.replace(\".csv\", \".pdf\"), dpi=300)\n    plt.close()\n\ndef calculate_robustness_acc(dataframe):\n    \"\"\"\n    Calculate the ACC metric based on robustness accuracies i.e.\n    accuracies after learning last task\n\n    Argument:\n    ---------\n      *load_path* (string) contains path to the .csv file with\n                  results in a dataframe shape, i.e. with columns:\n                  'after_learning_of_task', 'tested_task' (both\n                  integers) and 'accuracy' (float).\n\n    Returns:\n    --------\n      normed_robustness_acc (float): The computed ACC metric.\n    \"\"\"\n    if len(dataframe) != 0:\n        last_task = dataframe[\"after_learning_of_task\"].max()\n        accuracies_after_last_task = dataframe[dataframe[\"after_learning_of_task\"] == last_task]\n        unnormed_robustness_acc = accuracies_after_last_task[\"accuracy\"].sum()\n        normed_robustness_acc = unnormed_robustness_acc / (last_task + 1)\n    else:\n        normed_robustness_acc = 0\n\n    return normed_robustness_acc\n\n\ndef calculate_BWT(dataframe):\n    \"\"\"\n    Calculate the BWT metric based on accuracy data.\n\n    Argument:\n    ---------\n      *load_path* (string) contains path to the .csv file with\n                  results in a dataframe shape, i.e. with columns:\n                  'after_learning_of_task', 'tested_task' (both\n                  integers) and 'accuracy' (float).\n\n    Returns:\n    --------\n      BWT (float): The computed BWT metric.\n    \"\"\"\n\n    if len(dataframe) > 1:\n        last_task = dataframe[\"after_learning_of_task\"].max()\n        accuracies_after_last_task = dataframe[dataframe[\"after_learning_of_task\"] == last_task]\n        accuracies_of_newly_learned_tasks = dataframe[\n            dataframe[\"after_learning_of_task\"] == dataframe[\"tested_task\"]\n            ]\n        robustness_sum = accuracies_after_last_task[\n            accuracies_after_last_task[\"tested_task\"] != last_task\n            ][\"accuracy\"].sum()\n        freshness_sum = accuracies_of_newly_learned_tasks[\n            accuracies_of_newly_learned_tasks[\"tested_task\"] != last_task\n            ][\"accuracy\"].sum()\n        unnormed_BWT = robustness_sum - freshness_sum\n        BWT = unnormed_BWT / last_task\n    else:\n        BWT = 0\n\n    return BWT\n\n\ndef robustness_graph(load_path):\n    \"\"\"\n    Plot graph presenting ACC metric results for different learning tasks\n\n    Argument:\n    ---------\n      *load_path* (string) contains path to the .csv file with\n                  results in a dataframe shape, i.e. with columns:\n                  'after_learning_of_task', 'tested_task' (both\n                  integers) and 'accuracy' (float)\n    \"\"\"\n    dataframe = pd.read_csv(load_path, delimiter=\";\", index_col=0)\n    dataframe = dataframe.astype(\n        {\"after_learning_of_task\": \"int32\", \"tested_task\": \"int32\"}\n    )\n    max_task = dataframe[\"after_learning_of_task\"].max()\n    partial_robustness = []\n    for task in range(max_task+1):\n        partial_dataframe = dataframe[dataframe[\"after_learning_of_task\"] <= task]\n        partial_robustness.append(calculate_robustness_acc(partial_dataframe))\n\n    sns.lineplot(x=range(max_task+1), y=partial_robustness)\n    plt.xlabel(\"Task\")\n    plt.ylabel(\"Robustness\")\n    plt.title(\"Robustness during experiment\")\n    plt.tight_layout()\n    plt.savefig(load_path.replace(\".csv\", \"_ACC.pdf\"), dpi=300)\n    plt.close()\n\n\ndef bwt_graph(load_path):\n    \"\"\"\n    Plot graph presenting BWT metric results for different learning tasks\n\n    Argument:\n    ---------\n      *load_path* (string) contains path to the .csv file with\n                  results in a dataframe shape, i.e. with columns:\n                  'after_learning_of_task', 'tested_task' (both\n                  integers) and 'accuracy' (float)\n    \"\"\"\n    dataframe = pd.read_csv(load_path, delimiter=\";\", index_col=0)\n    dataframe = dataframe.astype(\n        {\"after_learning_of_task\": \"int32\", \"tested_task\": \"int32\"}\n    )\n\n    max_task = dataframe[\"after_learning_of_task\"].max()\n    partial_bwt = []\n    for task in range(max_task):\n        partial_dataframe = dataframe[dataframe[\"after_learning_of_task\"] <= task]\n        partial_bwt.append(calculate_BWT(partial_dataframe))\n\n    sns.lineplot(x=range(max_task), y=partial_bwt)\n    plt.tight_layout()\n    plt.savefig(load_path.replace(\".csv\", \"_BWT.pdf\"), dpi=300)\n    plt.close()\n\n\ndef train_single_task(\n    hypernetwork,\n    target_network,\n    criterion,\n    parameters,\n    dataset_list_of_tasks,\n    current_no_of_task,\n):\n    \"\"\"\n    Train two neural networks: a hypernetwork will generate a sparse\n    binary mask and the weights of the target neural network are multiplied by\n    this binary mask creating a sparse network. This module operates\n    on a single training task with a specific number.\n\n    Arguments:\n    ----------\n      *hypernetwork*: (hypnettorch.hnets module, e.g. mlp_hnet.MLP)\n                      a hypernetwork that generates weights for the target\n                      network\n      *target_network*: (hypnettorch.mnets module, e.g. mlp.MLP)\n                        a target network that finally will perform\n                        classification\n      *criterion*: (torch.nn module) implements a loss function,\n                   e.g. CrossEntropyLoss\n      *parameters*: (dictionary) contains necessary hyperparameters\n                    describing an experiment\n      *dataset_list_of_tasks*: a module containing list of tasks for the CL\n                               scenario, e.g. permuted_mnist.PermutedMNISTList\n      *current_no_of_task*: (int) specifies the number of currently solving task\n\n    Returns:\n    --------\n      *hypernetwork*: a modified module of hypernetwork\n      *target_network*: a modified module of the target network\n    \"\"\"\n    if parameters[\"optimizer\"] == \"adam\":\n        optimizer = torch.optim.Adam(\n            [*hypernetwork.parameters()],\n            lr=parameters[\"learning_rate\"],\n        )\n    elif parameters[\"optimizer\"] == \"rmsprop\":\n        optimizer = torch.optim.RMSprop(\n            [*hypernetwork.parameters()],\n            lr=parameters[\"learning_rate\"],\n        )\n    else:\n        raise ValueError(\"Wrong type of the selected optimizer!\")\n    if parameters[\"best_model_selection_method\"] == \"val_loss\":\n        # Store temporary best models to keep those with the highest\n        # validation accuracy.\n        best_hypernetwork = deepcopy(hypernetwork)\n        best_target_network = deepcopy(target_network)\n        best_val_accuracy = 0.0\n    elif parameters[\"best_model_selection_method\"] != \"last_model\":\n        raise ValueError(\n            \"Wrong value of best_model_selection_method parameter!\"\n        )\n    # Compute targets for the regularization part of loss before starting\n    # the training of a current task\n    hypernetwork.train()\n    print(f\"task: {current_no_of_task}\")\n    if current_no_of_task > 0:\n        regularization_targets = hreg.get_current_targets(\n            current_no_of_task, hypernetwork\n        )\n        previous_hnet_theta = None\n        previous_hnet_embeddings = None\n\n    if (parameters[\"target_network\"] == \"ResNet\") and parameters[\"use_batch_norm\"]:\n        use_batch_norm_memory = True\n    else:\n        use_batch_norm_memory = False\n    current_dataset_instance = dataset_list_of_tasks[current_no_of_task]\n    # If training through a given number of epochs is desired\n    # the number of iterations has to be calculated\n    if parameters[\"number_of_epochs\"] is not None:\n        (\n            no_of_iterations_per_epoch,\n            parameters[\"number_of_iterations\"],\n        ) = calculate_number_of_iterations(\n            current_dataset_instance.num_train_samples,\n            parameters[\"batch_size\"],\n            parameters[\"number_of_epochs\"],\n        )\n        # Scheduler can be set only when the number of epochs is given\n        if parameters[\"lr_scheduler\"]:\n            current_epoch = 0\n            plateau_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer,\n                \"max\",\n                factor=np.sqrt(0.1),\n                patience=5,\n                min_lr=0.5e-6,\n                cooldown=0,\n                verbose=True,\n            )\n    for iteration in range(parameters[\"number_of_iterations\"]):\n\n        current_batch = current_dataset_instance.next_train_batch(parameters[\"batch_size\"])\n        tensor_input = current_dataset_instance.input_to_torch_tensor(\n            current_batch[0], parameters[\"device\"], mode=\"train\"\n        )\n        tensor_output = current_dataset_instance.output_to_torch_tensor(\n            current_batch[1], parameters[\"device\"], mode=\"train\"\n        )\n        gt_output = tensor_output.max(dim=1)[1]\n        optimizer.zero_grad()\n        loss_norm_target_regularizer = 0.0\n\n        if \"weights\" in dir(target_network):\n            target_weights = target_network.weights\n        else:\n            target_weights = target_network\n\n        if parameters[\"target_network\"] == \"epsMLP\":\n            default_eps = target_network.epsilon\n            total_iterations = parameters[\"number_of_iterations\"]\n            inv_total_iterations = 1 / total_iterations\n            target_network.epsilon = iteration * inv_total_iterations * default_eps\n\n            prediction, eps_prediction = target_network.forward(\n                tensor_input, weights=target_weights\n            )\n\n            z_lower = prediction - eps_prediction.T\n            z_upper = prediction + eps_prediction.T\n            z = torch.where((nn.functional.one_hot(gt_output, prediction.size(-1))).bool(), z_lower, z_upper)\n\n            loss_spec = criterion(z, gt_output)\n            loss_fit = criterion(prediction, gt_output)\n            kappa = 1 - (iteration * inv_total_iterations * 0.5)\n\n            loss_current_task = kappa * loss_fit + (1 - kappa) * loss_spec\n            target_network.epsilon = default_eps\n        else:\n            prediction = target_network.forward(tensor_input, weights=target_weights)\n            loss_current_task = criterion(prediction, gt_output)\n            print(f' loss: {loss_current_task.item()}')\n        loss_regularization = 0.0\n        if current_no_of_task > 0:\n            loss_regularization = hreg.calc_fix_target_reg(\n                hypernetwork,\n                current_no_of_task,\n                targets=regularization_targets,\n                mnet=target_network,\n                prev_theta=previous_hnet_theta,\n                prev_task_embs=previous_hnet_embeddings,\n                inds_of_out_heads=None,\n                batch_size=-1,\n            )\n        append_row_to_file(\n            f'{parameters[\"saving_folder\"]}regularization_loss.csv',\n            f\"{current_no_of_task};{iteration};\"\n            f\"{loss_regularization};{loss_norm_target_regularizer}\",\n        )\n        loss = (\n            loss_current_task\n            + parameters[\"beta\"]\n            * loss_regularization\n            / max(1, current_no_of_task)\n            + parameters[\"lambda\"] * loss_norm_target_regularizer\n        )\n\n        loss.backward()\n        optimizer.step()\n\n        if parameters[\"number_of_epochs\"] is None:\n            condition = (iteration % 100 == 0) or (iteration == (parameters[\"number_of_iterations\"] - 1))\n        else:\n            condition = (\n                (iteration % 100 == 0)\n                or (iteration == (parameters[\"number_of_iterations\"] - 1))\n                or (((iteration + 1) % no_of_iterations_per_epoch) == 0)\n            )\n\n        if condition:\n            if parameters[\"number_of_epochs\"] is not None:\n                current_epoch = (iteration + 1) // no_of_iterations_per_epoch\n                print(f\"Current epoch: {current_epoch}\")\n            accuracy = calculate_accuracy(\n                current_dataset_instance,\n                target_network,\n                target_weights,\n                parameters={\n                    \"device\": parameters[\"device\"],\n                    \"use_batch_norm_memory\": use_batch_norm_memory,\n                    \"number_of_task\": current_no_of_task,\n                },\n                evaluation_dataset=\"validation\",\n            )\n            print(\n                f\"Task {current_no_of_task}, iteration: {iteration + 1},\"\n                f\" loss: {loss.item()}, validation accuracy: {accuracy}\"\n            )\n            # If the accuracy on the validation dataset is higher\n            # than previously\n            if parameters[\"best_model_selection_method\"] == \"val_loss\":\n                if accuracy > best_val_accuracy:\n                    best_val_accuracy = accuracy\n                    best_hypernetwork = deepcopy(hypernetwork)\n                    best_target_network = deepcopy(target_network)\n            if (\n                parameters[\"number_of_epochs\"] is not None\n                and parameters[\"lr_scheduler\"]\n                and (((iteration + 1) % no_of_iterations_per_epoch) == 0)\n            ):\n                print(\"Finishing the current epoch\")\n                plateau_scheduler.step(accuracy)\n\n    if parameters[\"best_model_selection_method\"] == \"val_loss\":\n        return best_hypernetwork, best_target_network\n    else:\n        return hypernetwork, target_network\n\ndef build_multiple_task_experiment(\n    dataset_list_of_tasks, parameters, use_chunks=False\n):\n    \"\"\"\n    Create a continual learning experiment with multiple tasks\n    for a given dataset.\n\n    Arguments:\n    ----------\n      *dataset_list_of_tasks*: a module containing list of tasks for the CL\n                               scenario, e.g. permuted_mnist.PermutedMNISTList\n      *parameters*: (dictionary) contains necessary hyperparameters\n                    describing an experiment\n      *use_chunks*: (Boolean value) optional argument, defines whether\n                    a hypernetwork should generate weights in chunks or not\n\n    Returns:\n    --------\n      *hypernetwork*: (hypnettorch.hnets module, e.g. mlp_hnet.MLP)\n                      a hypernetwork that generates weights for the target\n                      network\n      *target_network*: (hypnettorch.mnets module, e.g. mlp.MLP)\n                        a target network that finally will perform\n                        classification\n      *dataframe*: (Pandas Dataframe) contains results from consecutive\n                   evaluations for all previous tasks\n    \"\"\"\n    output_shape = list(dataset_list_of_tasks[0].get_train_outputs())[0].shape[0]\n    if parameters[\"target_network\"] == \"MLP\":\n        target_network = MLP(\n            n_in=parameters[\"input_shape\"],\n            n_out=output_shape,\n            hidden_layers=parameters[\"target_hidden_layers\"],\n            use_bias=parameters[\"use_bias\"],\n            no_weights=True,\n        ).to(parameters[\"device\"])\n    elif parameters[\"target_network\"] == \"epsMLP\":\n        target_network = epsMLP(\n            n_in=parameters[\"input_shape\"],\n            n_out=output_shape,\n            hidden_layers=parameters[\"target_hidden_layers\"],\n            use_bias=parameters[\"use_bias\"],\n            no_weights=True,\n            epsilon=0.01,\n        ).to(parameters[\"device\"])\n    elif parameters[\"target_network\"] == \"ResNet\":\n        target_network = ResNet(\n            in_shape=(parameters[\"input_shape\"], parameters[\"input_shape\"], 3),\n            use_bias=parameters[\"use_bias\"],\n            num_classes=output_shape,\n            n=parameters[\"resnet_number_of_layer_groups\"],\n            k=parameters[\"resnet_widening_factor\"],\n            no_weights=True,\n            use_batch_norm=parameters[\"use_batch_norm\"],\n            bn_track_stats=False,\n        ).to(parameters[\"device\"])\n    elif parameters[\"target_network\"] == \"ZenkeNet\":\n        if parameters[\"dataset\"] in [\"CIFAR100\", \"CIFAR100_FeCAM_setup\"]:\n            architecture = \"cifar\"\n        elif parameters[\"dataset\"] == \"TinyImageNet\":\n            architecture = \"tiny\"\n        else:\n            raise ValueError(\"This dataset is currently not implemented!\")\n        target_network = ZenkeNet(\n            in_shape=(parameters[\"input_shape\"], parameters[\"input_shape\"], 3),\n            num_classes=output_shape,\n            arch=architecture,\n            no_weights=True,\n        ).to(parameters[\"device\"])\n    # Create a hypernetwork based on the shape of the target network\n    if not use_chunks:\n        hypernetwork = HMLP(\n            target_network.param_shapes[0:],\n            uncond_in_size=0,\n            cond_in_size=parameters[\"embedding_size\"],\n            activation_fn=parameters[\"activation_function\"],\n            layers=parameters[\"hypernetwork_hidden_layers\"],\n            num_cond_embs=parameters[\"number_of_tasks\"],\n        ).to(parameters[\"device\"])\n    else:\n        hypernetwork = ChunkedHMLP(\n            target_shapes=target_network.param_shapes[0:],\n            chunk_size=parameters[\"chunk_size\"],\n            chunk_emb_size=parameters[\"chunk_emb_size\"],\n            cond_in_size=parameters[\"embedding_size\"],\n            activation_fn=parameters[\"activation_function\"],\n            layers=parameters[\"hypernetwork_hidden_layers\"],\n            num_cond_embs=parameters[\"number_of_tasks\"],\n        ).to(parameters[\"device\"])\n\n    criterion = nn.CrossEntropyLoss()\n    dataframe = pd.DataFrame(columns=[\"after_learning_of_task\", \"tested_task\", \"accuracy\"])\n\n    if (parameters[\"target_network\"] == \"ResNet\") and parameters[\"use_batch_norm\"]:\n        use_batch_norm_memory = True\n    else:\n        use_batch_norm_memory = False\n    hypernetwork.train()\n    target_network.train()\n    for no_of_task in range(parameters[\"number_of_tasks\"]):\n        target_network = torch.load(\"Results/grid_search/permuted_mnist/0/target_network_after_9_task.pt\")\n        hypernetwork = torch.load(\"Results/grid_search/permuted_mnist/0/hypernetwork_after_9_task.pt\")\n\n        if no_of_task <= (parameters[\"number_of_tasks\"] - 1):\n            # Save current state of networks\n            write_pickle_file(\n                f'{parameters[\"saving_folder\"]}/'\n                f\"hypernetwork_after_{no_of_task}_task\",\n                hypernetwork.weights,\n            )\n            write_pickle_file(\n                f'{parameters[\"saving_folder\"]}/'\n                f\"target_network_after_{no_of_task}_task\",\n                target_network.weights,\n            )\n        dataframe = evaluate_previous_tasks(\n            hypernetwork,\n            target_network,\n            dataframe,\n            dataset_list_of_tasks,\n            parameters[\"sparsity_parameter\"],\n            parameters={\n                \"device\": parameters[\"device\"],\n                \"use_batch_norm_memory\": use_batch_norm_memory,\n                \"number_of_task\": no_of_task,\n            },\n        )\n        dataframe = dataframe.astype(\n            {\"after_learning_of_task\": \"int\", \"tested_task\": \"int\"}\n        )\n\n        print(f\"Robustness accuracy: {calculate_robustness_acc(dataframe)}\")\n        print(f\"BWT accuracy: {calculate_BWT(dataframe)}\")\n\n        dataframe.to_csv(\n            f'{parameters[\"saving_folder\"]}/'\n            f'results_{parameters[\"name_suffix\"]}.csv',\n            sep=\";\",\n        )\n\n    return hypernetwork, target_network, dataframe\n\n\ndef main_running_experiments(path_to_datasets, parameters):\n    \"\"\"\n    Perform a series of experiments based on the hyperparameters.\n\n    Arguments:\n    ----------\n      *path_to_datasets*: (str) path to files with datasets\n      *parameters*: (dict) contains multiple experiment hyperparameters\n\n    Returns learned hypernetwork, target network and a dataframe\n    with single results.\n    \"\"\"\n    if parameters[\"dataset\"] == \"PermutedMNIST\":\n        dataset_tasks_list = prepare_permuted_mnist_tasks(\n            path_to_datasets,\n            parameters[\"input_shape\"],\n            parameters[\"number_of_tasks\"],\n            parameters[\"padding\"],\n            parameters[\"no_of_validation_samples\"],\n        )\n    elif parameters[\"dataset\"] == \"CIFAR100\":\n        dataset_tasks_list = prepare_split_cifar100_tasks(\n            path_to_datasets,\n            validation_size=parameters[\"no_of_validation_samples\"],\n            use_augmentation=parameters[\"augmentation\"],\n        )\n    elif parameters[\"dataset\"] == \"SplitMNIST\":\n        dataset_tasks_list = prepare_split_mnist_tasks(\n            path_to_datasets,\n            validation_size=parameters[\"no_of_validation_samples\"],\n            use_augmentation=parameters[\"augmentation\"],\n            number_of_tasks=parameters[\"number_of_tasks\"],\n        )\n    elif parameters[\"dataset\"] == \"TinyImageNet\":\n        dataset_tasks_list = prepare_tinyimagenet_tasks(\n            path_to_datasets,\n            seed=parameters[\"seed\"],\n            validation_size=parameters[\"no_of_validation_samples\"],\n            number_of_tasks=parameters[\"number_of_tasks\"],\n        )\n    elif parameters[\"dataset\"] == \"CIFAR100_FeCAM_setup\":\n        dataset_tasks_list = prepare_split_cifar100_tasks_aka_FeCAM(\n            path_to_datasets,\n            number_of_tasks=parameters[\"number_of_tasks\"],\n            no_of_validation_samples_per_class=parameters[\n                \"no_of_validation_samples_per_class\"\n            ],\n            use_augmentation=parameters[\"augmentation\"],\n        )\n    else:\n        raise ValueError(\"Wrong name of the dataset!\")\n\n    hypernetwork, target_network, dataframe = build_multiple_task_experiment(\n        dataset_tasks_list, parameters, use_chunks=parameters[\"use_chunks\"]\n    )\n    # Calculate statistics of grid search results\n    no_of_last_task = parameters[\"number_of_tasks\"] - 1\n    accuracies = dataframe.loc[\n        dataframe[\"after_learning_of_task\"] == no_of_last_task\n    ][\"accuracy\"].values\n    row_with_results = (\n        f\"{dataset_tasks_list[0].get_identifier()};\"\n        f'{parameters[\"augmentation\"]};'\n        f'{parameters[\"embedding_size\"]};'\n        f'{parameters[\"seed\"]};'\n        f'{str(parameters[\"hypernetwork_hidden_layers\"]).replace(\" \", \"\")};'\n        f'{parameters[\"use_chunks\"]};{parameters[\"chunk_emb_size\"]};'\n        f'{parameters[\"target_network\"]};'\n        f'{str(parameters[\"target_hidden_layers\"]).replace(\" \", \"\")};'\n        f'{parameters[\"resnet_number_of_layer_groups\"]};'\n        f'{parameters[\"resnet_widening_factor\"]};'\n        f'{parameters[\"norm_regularizer_masking\"]};'\n        f'{parameters[\"best_model_selection_method\"]};'\n        f'{parameters[\"optimizer\"]};'\n        f'{parameters[\"activation_function\"]};'\n        f'{parameters[\"learning_rate\"]};{parameters[\"batch_size\"]};'\n        f'{parameters[\"beta\"]};{parameters[\"sparsity_parameter\"]};'\n        f'{parameters[\"norm\"]};{parameters[\"lambda\"]};'\n        f\"{np.mean(accuracies)};{np.std(accuracies)}\"\n    )\n    append_row_to_file(\n        f'{parameters[\"grid_search_folder\"]}'\n        f'{parameters[\"summary_results_filename\"]}.csv',\n        row_with_results,\n    )\n\n    load_path = (\n        f'{parameters[\"saving_folder\"]}/'\n        f'results_{parameters[\"name_suffix\"]}.csv'\n    )\n    plot_heatmap(load_path)\n    robustness_graph(load_path)\n    bwt_graph(load_path)\n\n    return hypernetwork, target_network, dataframe\n\n\nif __name__ == \"__main__\":\n    path_to_datasets = \"./Data\"\n    dataset = \"PermutedMNIST\"\n    # 'PermutedMNIST', 'CIFAR100', 'SplitMNIST', 'TinyImageNet',\n    # 'CIFAR100_FeCAM_setup'\n    part = 1\n    create_grid_search = True\n    if create_grid_search:\n        summary_results_filename = \"grid_search_results\"\n    else:\n        summary_results_filename = \"summary_results\"\n    hyperparameters = set_hyperparameters(\n        dataset, grid_search=create_grid_search, part=part\n    )\n    header = (\n        \"dataset_name;augmentation;embedding_size;seed;hypernetwork_hidden_layers;\"\n        \"use_chunks;chunk_emb_size;target_network;target_hidden_layers;\"\n        \"layer_groups;widening;norm_regularizer_masking;final_model;optimizer;\"\n        \"hypernet_activation_function;learning_rate;batch_size;beta;\"\n        \"sparsity;norm;lambda;mean_accuracy;std_accuracy\"\n    )\n    append_row_to_file(\n        f'{hyperparameters[\"saving_folder\"]}{summary_results_filename}.csv',\n        header,\n    )\n\n    for no, elements in enumerate(\n        product(\n            hyperparameters[\"embedding_sizes\"],\n            hyperparameters[\"learning_rates\"],\n            hyperparameters[\"betas\"],\n            hyperparameters[\"hypernetworks_hidden_layers\"],\n            hyperparameters[\"sparsity_parameters\"],\n            hyperparameters[\"lambdas\"],\n            hyperparameters[\"batch_sizes\"],\n            hyperparameters[\"norm_regularizer_masking_opts\"],\n            hyperparameters[\"seed\"],\n        )\n    ):\n        embedding_size = elements[0]\n        learning_rate = elements[1]\n        beta = elements[2]\n        hypernetwork_hidden_layers = elements[3]\n        sparsity_parameter = elements[4]\n        lambda_par = elements[5]\n        batch_size = elements[6]\n        norm_regularizer_masking = elements[7]\n        # Of course, seed is not optimized, but it is easier to prepare experiments for multiple seeds in such a way\n        seed = elements[8]\n\n        parameters = {\n            \"input_shape\": hyperparameters[\"shape\"],\n            \"augmentation\": hyperparameters[\"augmentation\"],\n            \"number_of_tasks\": hyperparameters[\"number_of_tasks\"],\n            \"dataset\": dataset,\n            \"seed\": seed,\n            \"hypernetwork_hidden_layers\": hypernetwork_hidden_layers,\n            \"activation_function\": hyperparameters[\"activation_function\"],\n            \"norm_regularizer_masking\": norm_regularizer_masking,\n            \"use_chunks\": hyperparameters[\"use_chunks\"],\n            \"chunk_size\": hyperparameters[\"chunk_size\"],\n            \"chunk_emb_size\": hyperparameters[\"chunk_emb_size\"],\n            \"target_network\": hyperparameters[\"target_network\"],\n            \"target_hidden_layers\": hyperparameters[\"target_hidden_layers\"],\n            \"resnet_number_of_layer_groups\": hyperparameters[\n                \"resnet_number_of_layer_groups\"\n            ],\n            \"resnet_widening_factor\": hyperparameters[\"resnet_widening_factor\"],\n            \"adaptive_sparsity\": hyperparameters[\"adaptive_sparsity\"],\n            \"sparsity_parameter\": sparsity_parameter,\n            \"learning_rate\": learning_rate,\n            \"best_model_selection_method\": hyperparameters[\n                \"best_model_selection_method\"\n            ],\n            \"lr_scheduler\": hyperparameters[\"lr_scheduler\"],\n            \"batch_size\": batch_size,\n            \"number_of_epochs\": hyperparameters[\"number_of_epochs\"],\n            \"number_of_iterations\": hyperparameters[\"number_of_iterations\"],\n            \"no_of_validation_samples_per_class\": hyperparameters[\n                \"no_of_validation_samples_per_class\"\n            ],\n            \"embedding_size\": embedding_size,\n            \"norm\": hyperparameters[\"norm\"],\n            \"lambda\": lambda_par,\n            \"optimizer\": hyperparameters[\"optimizer\"],\n            \"beta\": beta,\n            \"padding\": hyperparameters[\"padding\"],\n            \"use_bias\": hyperparameters[\"use_bias\"],\n            \"use_batch_norm\": hyperparameters[\"use_batch_norm\"],\n            \"device\": hyperparameters[\"device\"],\n            \"saving_folder\": f'{hyperparameters[\"saving_folder\"]}{no}/',\n            \"grid_search_folder\": hyperparameters[\"saving_folder\"],\n            \"save_masks\": hyperparameters[\"save_consecutive_masks\"],\n            \"name_suffix\": f\"mask_sparsity_{sparsity_parameter}\",\n            \"summary_results_filename\": summary_results_filename,\n        }\n        if \"no_of_validation_samples\" in hyperparameters:\n            parameters[\"no_of_validation_samples\"] = hyperparameters[\n                \"no_of_validation_samples\"\n            ]\n\n        os.makedirs(parameters[\"saving_folder\"], exist_ok=True)\n        # start_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        save_parameters(\n            parameters[\"saving_folder\"],\n            parameters,\n            name=f'parameters_{parameters[\"name_suffix\"]}.csv',\n        )\n\n        # Important! Seed is set before the preparation of the dataset!\n        if seed is not None:\n            set_seed(seed)\n\n        hypernetwork, target_network, dataframe = main_running_experiments(\n            path_to_datasets, parameters\n        )\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	(revision 9bcf188f17f73fe837356539a1ef3df5f2cd5485)
+++ b/main.py	(date 1739546997710)
@@ -31,7 +31,6 @@
     prepare_tinyimagenet_tasks,
 )
 
-from autoattack import AutoAttack
 
 def set_seed(value):
     """
@@ -114,6 +113,7 @@
     total_no_of_iterations = int(no_of_iterations_per_epoch * number_of_epochs)
     return no_of_iterations_per_epoch, total_no_of_iterations
 
+
 def calculate_accuracy(data, target_network, weights, parameters, evaluation_dataset):
     """
     Calculate accuracy for a given dataset using a selected network
@@ -140,13 +140,8 @@
     --------
       torch.Tensor: Accuracy (percentage) for the selected setting.
     """
-    assert (
-        parameters["use_batch_norm_memory"]
-        and parameters["number_of_task"]is not None
-    ) or not parameters["use_batch_norm_memory"]
-    assert evaluation_dataset in ["validation", "test"]
+
     target_network.eval()
-    torch.no_grad()
     if evaluation_dataset == "validation":
         input_data = data.get_val_inputs()
         output_data = data.get_val_outputs()
@@ -160,11 +155,7 @@
     gt_classes = test_output.max(dim=1)[1]
 
     if parameters["use_batch_norm_memory"]:
-        logits = target_network.forward(
-            test_input,
-            weights=weights,
-            condition=parameters["number_of_task"],
-        )
+        logits = target_network.forward(test_input, weights=weights, condition=parameters["number_of_task"])
     else:
         logits = target_network.forward(test_input, weights=weights)
 
@@ -175,98 +166,17 @@
 
     if evaluation_dataset == "test":
         # FGSM, PGD, AutoAttack, None
-
-        attack_method = "AutoAttack"
-        if attack_method == None:
-            pass
-            # accuracy = (
-            #                    torch.sum(gt_classes == predictions).float() / gt_classes.numel()
-            #            ) * 100.0
-            # return accuracy
-        elif attack_method == "FGSM":
-            ksi = 25 / 255 # attack strength
-            criterion = nn.CrossEntropyLoss()
-            loss = criterion(logits, gt_classes)
-            target_network.zero_grad()
-            loss.backward()
-
-            perturbation = torch.clamp(ksi * test_input.grad.data.sign(), -0.01,0.01)
-
-            # data_grad = test_input.grad.data
-            # signed_grad = data_grad.sign()
-            # perturbation = ksi * signed_grad
-            perturbed_test_input = test_input + perturbation
-            perturbed_test_input = torch.clamp(perturbed_test_input, 0, 1)
-
-            assert torch.max(torch.abs(perturbed_test_input - test_input)) <= 0.01
-
-            perturbed_output, _ = target_network.forward(perturbed_test_input, weights=weights)
-            perturbed_pred = perturbed_output.max(dim=1)[1]
-            perturbed_acc = (torch.sum(gt_classes == perturbed_pred).float() / gt_classes.numel()) * 100
-
-            return perturbed_acc
-        elif attack_method == 'PGD':
-            ksi = 40 / 255
-            alpha = 1
-            random_start = False
-            num_iteration = min(ksi + 4, 1.25 * ksi)
-            criterion = nn.CrossEntropyLoss()
-
-            perturbed_input = test_input.clone().detach()
-            perturbed_input.requires_grad = True
-
-            if random_start:
-                perturbed_input += torch.empty_like(perturbed_input).uniform_(-ksi, ksi)
-                perturbed_input = torch.clamp(perturbed_input, 0, 1)
-
-            for it in range(num_iteration):
-                perturbed_input.requires_grad_()
-                outputs, _ = target_network.forward(perturbed_input, weights=weights)
-
-                loss = criterion(outputs, gt_classes)
-                target_network.zero_grad()
-                loss.backward(retain_graph=True)
-
-                perturbation = alpha * perturbed_input.grad.sign()
-                perturbed_input = perturbed_input + perturbation
-                perturbed_input = torch.clamp(perturbed_input, -ksi, ksi).detach()
-
-                perturbed_input.requires_grad_()
-
-            perturbed_output, _ = target_network.forward(perturbed_input, weights=weights)
-            perturbed_pred = perturbed_output.max(dim=1)[1]
-            perturbed_acc = (torch.sum(gt_classes == perturbed_pred).float() / gt_classes.numel()) * 100.0
-
-            return perturbed_acc
-        elif attack_method == 'AutoAttack':
-            ksi = 20/255
-            adversary = AutoAttack(
-                lambda x: target_network.forward(x, weights=weights)[0],
-                norm='L2',
-                eps=ksi,
-                version='standard',
-                device=parameters["device"]
-            )
-            perturbed_input = test_input.clone().detach()
-            x_adv = adversary.run_standard_evaluation(perturbed_input, gt_classes, bs=batch_size)
-
-            perturbed_output, _ = target_network.forward(x_adv, weights=weights)
-            perturbed_pred = perturbed_output.max(dim=1)[1]
-            perturbed_acc = (torch.sum(gt_classes == perturbed_pred).float() / gt_classes.numel()) * 100.0
-
-            return perturbed_acc
+        pass
 
     accuracy = torch.sum(gt_classes == predictions).float() / gt_classes.numel() * 100.0
     return accuracy
 
 
-
 def evaluate_previous_tasks(
     hypernetwork,
     target_network,
     dataframe_results,
     list_of_permutations,
-    sparsity_parameter,
     parameters,
 ):
     """
@@ -314,16 +224,14 @@
     for task in range(parameters["number_of_task"] + 1):
         currently_tested_task = list_of_permutations[task]
         # Generate weights of the target network
-        hypernetwork_weights = hypernetwork.forward(cond_id=task)
         if "weights" in dir(target_network):
-            target_network_weights = target_network.weights
+            target_weights = target_network.weights
         else:
-            target_network_weights = target_network
-
+            target_weights = target_network
         accuracy = calculate_accuracy(
             currently_tested_task,
             target_network,
-            target_network_weights,
+            target_weights,
             parameters=parameters,
             evaluation_dataset="test",
         )
@@ -604,6 +512,7 @@
         )
         gt_output = tensor_output.max(dim=1)[1]
         optimizer.zero_grad()
+
         loss_norm_target_regularizer = 0.0
 
         if "weights" in dir(target_network):
@@ -809,29 +718,23 @@
     else:
         use_batch_norm_memory = False
     hypernetwork.train()
-    target_network.train()
     for no_of_task in range(parameters["number_of_tasks"]):
         target_network = torch.load("Results/grid_search/permuted_mnist/0/target_network_after_9_task.pt")
         hypernetwork = torch.load("Results/grid_search/permuted_mnist/0/hypernetwork_after_9_task.pt")
-
+        #TODO change it to save all checkpoints
         if no_of_task <= (parameters["number_of_tasks"] - 1):
+            print('x')
             # Save current state of networks
-            write_pickle_file(
-                f'{parameters["saving_folder"]}/'
-                f"hypernetwork_after_{no_of_task}_task",
-                hypernetwork.weights,
-            )
-            write_pickle_file(
-                f'{parameters["saving_folder"]}/'
-                f"target_network_after_{no_of_task}_task",
-                target_network.weights,
-            )
+            # write_pickle_file(
+            #     f'{parameters["saving_folder"]}/'
+            #     f"hypernetwork_after_{no_of_task}_task",
+            #     hypernetwork.weights,
+            # )
         dataframe = evaluate_previous_tasks(
             hypernetwork,
             target_network,
             dataframe,
             dataset_list_of_tasks,
-            parameters["sparsity_parameter"],
             parameters={
                 "device": parameters["device"],
                 "use_batch_norm_memory": use_batch_norm_memory,
@@ -944,9 +847,6 @@
         f'{parameters["saving_folder"]}/'
         f'results_{parameters["name_suffix"]}.csv'
     )
-    plot_heatmap(load_path)
-    robustness_graph(load_path)
-    bwt_graph(load_path)
 
     return hypernetwork, target_network, dataframe
 
@@ -983,7 +883,6 @@
             hyperparameters["learning_rates"],
             hyperparameters["betas"],
             hyperparameters["hypernetworks_hidden_layers"],
-            hyperparameters["sparsity_parameters"],
             hyperparameters["lambdas"],
             hyperparameters["batch_sizes"],
             hyperparameters["norm_regularizer_masking_opts"],
@@ -994,12 +893,11 @@
         learning_rate = elements[1]
         beta = elements[2]
         hypernetwork_hidden_layers = elements[3]
-        sparsity_parameter = elements[4]
-        lambda_par = elements[5]
-        batch_size = elements[6]
-        norm_regularizer_masking = elements[7]
+        lambda_par = elements[4]
+        batch_size = elements[5]
+        norm_regularizer_masking = elements[6]
         # Of course, seed is not optimized, but it is easier to prepare experiments for multiple seeds in such a way
-        seed = elements[8]
+        seed = elements[7]
 
         parameters = {
             "input_shape": hyperparameters["shape"],
@@ -1020,7 +918,6 @@
             ],
             "resnet_widening_factor": hyperparameters["resnet_widening_factor"],
             "adaptive_sparsity": hyperparameters["adaptive_sparsity"],
-            "sparsity_parameter": sparsity_parameter,
             "learning_rate": learning_rate,
             "best_model_selection_method": hyperparameters[
                 "best_model_selection_method"
@@ -1044,7 +941,7 @@
             "saving_folder": f'{hyperparameters["saving_folder"]}{no}/',
             "grid_search_folder": hyperparameters["saving_folder"],
             "save_masks": hyperparameters["save_consecutive_masks"],
-            "name_suffix": f"mask_sparsity_{sparsity_parameter}",
+            "name_suffix": f"mask_sparsity",
             "summary_results_filename": summary_results_filename,
         }
         if "no_of_validation_samples" in hyperparameters:
@@ -1064,6 +961,4 @@
         if seed is not None:
             set_seed(seed)
 
-        hypernetwork, target_network, dataframe = main_running_experiments(
-            path_to_datasets, parameters
-        )
+        hypernetwork, target_network, dataframe = main_running_experiments(path_to_datasets, parameters)
Index: datasets.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport numpy as np\nimport torch\nfrom hypnettorch.data.special import permuted_mnist\nfrom hypnettorch.data.special.split_cifar import SplitCIFAR100Data\nfrom hypnettorch.data.special.split_mnist import get_split_mnist_handlers\nfrom TinyImageNet import TinyImageNet\nfrom CIFAR100_FeCAM import SplitCIFAR100Data_FeCAM\nimport attacks\n\ndef generate_random_permutations(\n    shape_of_data_instance, number_of_permutations\n):\n    \"\"\"\n    Prepare a list of random permutations of the selected shape\n    for continual learning tasks.\n\n    Arguments:\n    ----------\n      *shape_of_data_instance*: a number defining shape of the dataset\n      *number_of_permutations*: int, a number of permutations that will\n                                be prepared; it corresponds to the total\n                                number of tasks\n      *seed*: int, optional argument, default: None\n              if one would get deterministic results\n    \"\"\"\n    list_of_permutations = []\n    for _ in range(number_of_permutations):\n        list_of_permutations.append(\n            np.random.permutation(shape_of_data_instance)\n        )\n    return list_of_permutations\n\n\ndef prepare_split_cifar100_tasks(\n    datasets_folder, validation_size, use_augmentation, use_cutout=False\n):\n    \"\"\"\n    Prepare a list of 10 tasks with 10 classes per each task.\n    i-th task, where i in {0, 1, ..., 9} will store samples\n    from classes {10*i, 10*i + 1, ..., 10*i + 9}.\n\n    Arguments:\n    ----------\n      *datasets_folder*: (string) Defines a path in which CIFAR-100\n                         is stored / will be downloaded\n      *validation_size*: (int) The number of validation samples\n      *use_augmentation*: (Boolean) potentially applies\n                          a data augmentation method from\n                          hypnettorch\n      *use_cutout*: (optional Boolean) in the positive case it applies\n                    'apply_cutout' option form 'torch_input_transforms'.\n    \"\"\"\n    handlers = []\n    for i in range(0, 100, 10):\n        handlers.append(\n            SplitCIFAR100Data(\n                datasets_folder,\n                use_one_hot=True,\n                validation_size=validation_size,\n                use_data_augmentation=use_augmentation,\n                use_cutout=use_cutout,\n                labels=range(i, i + 10),\n            )\n        )\n    return handlers\n\n\ndef prepare_split_cifar100_tasks_aka_FeCAM(\n    datasets_folder,\n    number_of_tasks,\n    no_of_validation_samples_per_class,\n    use_augmentation,\n    use_cutout=False,\n):\n    \"\"\"\n    Prepare a list of 5, 10 or 20 incremental tasks with 20, 10 or 5 classes,\n    respectively, per each task. Furthermore, the first task contains\n    a higher number of classes, i.e. 50 or 40. Therefore, in these cases,\n    the total number of tasks is equal to 6, 11 or 21.\n    Also, there is a possibility of 5 tasks with 20 classes per each.\n    The order of classes is the same like in FeCAM, also the scenarios\n    are constructed in such a way to enable a fair comparison with FeCAM\n\n    Arguments:\n    ----------\n      *datasets_folder*: (string) Defines a path in which CIFAR-100\n                         is stored / will be downloaded\n      *number_of_tasks* (int) Defines how many continual learning tasks\n                        will be created. Possible options: 6, 11 or 21\n      *no_of_validation_samples_per_class*: (int) The number of validation\n                                            samples in a single class\n      *use_augmentation*: (Boolean) potentially applies\n                          a data augmentation method from\n                          hypnettorch\n      *use_cutout*: (optional Boolean) in the positive case it applies\n                    'apply_cutout' option form 'torch_input_transforms'.\n    \"\"\"\n    # FeCAM considered four scenarios: 5, 10 and 20 incremental tasks\n    # and 5 tasks with the equal number of classes\n    assert number_of_tasks in [5, 6, 11, 21]\n    # The order of image classes in the case of FeCAM was not 0-10, 11-20, etc.,\n    # but it was chosen randomly by the authors, and was at follows:\n    class_orders = [\n        87, 0, 52, 58, 44, 91, 68, 97, 51, 15,\n        94, 92, 10, 72, 49, 78, 61, 14, 8, 86,\n        84, 96, 18, 24, 32, 45, 88, 11, 4, 67,\n        69, 66, 77, 47, 79, 93, 29, 50, 57, 83,\n        17, 81, 41, 12, 37, 59, 25, 20, 80, 73,\n        1, 28, 6, 46, 62, 82, 53, 9, 31, 75,\n        38, 63, 33, 74, 27, 22, 36, 3, 16, 21,\n        60, 19, 70, 90, 89, 43, 5, 42, 65, 76,\n        40, 30, 23, 85, 2, 95, 56, 48, 71, 64,\n        98, 13, 99, 7, 34, 55, 54, 26, 35, 39\n    ]\n    # Incremental tasks from Table I, FeCAM\n    if number_of_tasks == 6:\n        numbers_of_classes_per_tasks = [50]\n        numbers_of_classes_per_tasks.extend([10 for i in range(5)])\n    elif number_of_tasks == 11:\n        numbers_of_classes_per_tasks = [50]\n        numbers_of_classes_per_tasks.extend([5 for i in range(10)])\n    elif number_of_tasks == 21:\n        numbers_of_classes_per_tasks = [40]\n        numbers_of_classes_per_tasks.extend([3 for i in range(20)])\n    # Tasks with the equal number of elements, Table V, FeCAM\n    elif number_of_tasks == 5:\n        numbers_of_classes_per_tasks = [20 for i in range(5)]\n\n    handlers = []\n    for i in range(len(numbers_of_classes_per_tasks)):\n        current_number_of_tasks = numbers_of_classes_per_tasks[i]\n        validation_size = (\n            no_of_validation_samples_per_class * current_number_of_tasks\n        )\n        handlers.append(\n            SplitCIFAR100Data_FeCAM(\n                datasets_folder,\n                use_one_hot=True,\n                validation_size=validation_size,\n                use_data_augmentation=use_augmentation,\n                use_cutout=use_cutout,\n                labels=class_orders[\n                    (i * current_number_of_tasks) : (\n                        (i + 1) * current_number_of_tasks\n                    )\n                ],\n            )\n        )\n    return handlers\n\n\ndef prepare_tinyimagenet_tasks(\n    datasets_folder, seed, validation_size=250, number_of_tasks=40\n):\n    \"\"\"\n    Prepare a list of *number_of_tasks* tasks related\n    to the TinyImageNet dataset according to the WSN setup.\n\n    Arguments:\n    ----------\n      *datasets_folder*: (string) Defines a path in which TinyImageNet\n                         is stored / will be downloaded\n      *seed*: (int) Necessary for the preparation of random permutation\n              of the order of classes in consecutive tasks.\n      *validation_size*: (optional int) defines the number of validation\n                         samples in each task, by default it is 250 like\n                         in the case of WSN\n      *number_of_tasks*: (optional int) defines the number of continual\n                         learning tasks (by default: 40)\n\n    Returns a list of TinyImageNet objects.\n    \"\"\"\n    # Set randomly the order of classes\n    rng = np.random.default_rng(seed)\n    class_permutation = rng.permutation(200)\n    # 40 classification tasks with 5 classes in each\n    handlers = []\n    for i in range(0, 5 * number_of_tasks, 5):\n        current_labels = class_permutation[i : (i + 5)]\n        print(f\"Order of classes in the current task: {current_labels}\")\n        handlers.append(\n            TinyImageNet(\n                data_path=datasets_folder,\n                validation_size=validation_size,\n                use_one_hot=True,\n                labels=current_labels,\n            )\n        )\n    return handlers\n\n\ndef prepare_permuted_mnist_tasks(\n    datasets_folder, input_shape, number_of_tasks, padding, validation_size\n):\n    \"\"\"\n    Prepare a list of *number_of_tasks* tasks related\n    to the PermutedMNIST dataset.\n\n    Arguments:\n    ----------\n      *datasets_folder*: (string) Defines a path in which MNIST dataset\n                         is stored / will be downloaded\n      *input_shape*: (int) a number defining shape of the dataset\n      *validation_size*: (int) The number of validation samples\n\n    Returns a list of PermutedMNIST objects.\n    \"\"\"\n    permutations = generate_random_permutations(input_shape, number_of_tasks)\n    return permuted_mnist.PermutedMNISTList(\n        permutations,\n        datasets_folder,\n        use_one_hot=True,\n        padding=padding,\n        validation_size=validation_size,\n    )\n\n\ndef prepare_split_mnist_tasks(\n    datasets_folder, validation_size, use_augmentation, number_of_tasks=5\n):\n    \"\"\"\n    Prepare a list of *number_of_tasks* tasks related\n    to the SplitMNIST dataset. By default, it should be\n    5 task containing consecutive pairs of classes:\n    [0, 1], [2, 3], [4, 5], [6, 7] and [8, 9].\n\n    Arguments:\n    ----------\n      *datasets_folder*: (string) Defines a path in which MNIST dataset\n                         is stored / will be downloaded\n      *validation_size*: (int) The number of validation samples\n      *use_augmentation*: (bool) defines whether dataset augmentation\n                          will be applied\n      *number_of_tasks* (int) a number defining the number of learning\n                        tasks, by default 5.\n\n    Returns a list of SplitMNIST objects.\n    \"\"\"\n    return get_split_mnist_handlers(\n        datasets_folder,\n        use_one_hot=True,\n        validation_size=validation_size,\n        num_classes_per_task=2,\n        num_tasks=number_of_tasks,\n        use_torch_augmentation=use_augmentation,\n    )\n\n\ndef set_hyperparameters(dataset, grid_search=False, part=0):\n    \"\"\"\n    Set hyperparameters of the experiments, both in the case of grid search\n    optimization and a single network run.\n\n    Arguments:\n    ----------\n      *dataset*: 'PermutedMNIST', 'SplitMNIST' or 'CIFAR100'\n      *grid_search*: (Boolean optional) defines whether a hyperparameter\n                     optimization should be performed or hyperparameters\n                     for just a single run have to be returned\n      *part* (only for SplitMNIST or CIFAR100!) selects a subset\n             of hyperparameters for optimization (by default 0)\n\n    Returns a dictionary with necessary hyperparameters.\n    \"\"\"\n    if dataset == \"PermutedMNIST\":\n        # us or paper\n        param_owner = 'paper'\n        if param_owner == 'us':\n            if grid_search:\n                hyperparams = {\n                    \"embedding_sizes\": [24],\n                    \"learning_rates\": [0.001],\n                    \"batch_sizes\": [128],\n                    \"norm_regularizer_masking_opts\": [True, False],\n                    \"betas\": [0.001, 0.0005, 0.005],\n                    \"hypernetworks_hidden_layers\": [[100, 100]],\n                    \"sparsity_parameters\": [0],\n                    \"lambdas\": [0.001, 0.0005],\n                    \"best_model_selection_method\": \"val_loss\",\n                    \"saving_folder\": \"./Results/grid_search/permuted_mnist/\",\n                    # not for optimization, just for multiple cases\n                    \"seed\": [1, 2, 3, 4, 5],\n                }\n\n            else:\n                # Best hyperparameters\n                hyperparams = {\n                    \"seed\": [1, 2, 3, 4, 5],\n                    \"embedding_sizes\": [24],\n                    \"sparsity_parameters\": [0],\n                    \"learning_rates\": [0.001],\n                    \"batch_sizes\": [128],\n                    \"betas\": [0.0005],\n                    \"lambdas\": [0.001],\n                    \"norm_regularizer_masking_opts\": [True],\n                    \"hypernetworks_hidden_layers\": [[100, 100]],\n                    \"best_model_selection_method\": \"last_model\",\n                    \"saving_folder\": \"./Results/permuted_mnist_best_hyperparams/\",\n                }\n\n            # Both in the grid search and individual runs\n            hyperparams[\"lr_scheduler\"] = False\n            hyperparams[\"number_of_iterations\"] = 5000\n            hyperparams[\"number_of_epochs\"] = None\n            hyperparams[\"no_of_validation_samples\"] = 5000\n            hyperparams[\"no_of_validation_samples_per_class\"] = 500\n            hyperparams[\"target_hidden_layers\"] = [1000, 1000]\n            hyperparams[\"target_network\"] = \"epsMLP\"\n            hyperparams[\"resnet_number_of_layer_groups\"] = None\n            hyperparams[\"resnet_widening_factor\"] = None\n            hyperparams[\"optimizer\"] = \"adam\"\n            hyperparams[\"chunk_size\"] = 100\n            hyperparams[\"chunk_emb_size\"] = 8\n            hyperparams[\"use_chunks\"] = False\n            hyperparams[\"adaptive_sparsity\"] = True\n            hyperparams[\"use_batch_norm\"] = False\n            # Directly related to the MNIST dataset\n            hyperparams[\"padding\"] = 2\n            hyperparams[\"shape\"] = (28 + 2 * hyperparams[\"padding\"]) ** 2\n            hyperparams[\"number_of_tasks\"] = 10\n            hyperparams[\"augmentation\"] = False\n        # paper\n        else:\n            if grid_search:\n                hyperparams = {\n                    \"embedding_sizes\": [24],\n                    \"learning_rates\": [0.001],\n                    \"batch_sizes\": [32],\n                    \"norm_regularizer_masking_opts\": [True, False],\n                    \"betas\": [0.001, 0.0005, 0.005, 0.0001, 0.01, 0.05],\n                    \"hypernetworks_hidden_layers\": [[100, 100]],\n                    \"sparsity_parameters\": [0],\n                    \"lambdas\": [0.001, 0.0005, 0.0001, 0.005, 0.01, 0.05],\n                    \"best_model_selection_method\": \"val_loss\",\n                    \"saving_folder\": \"./Results/grid_search/permuted_mnist/\",\n                    # not for optimization, just for multiple cases\n                    \"seed\": [1, 2, 3, 4, 5],\n                }\n\n            else:\n                # Best hyperparameters\n                hyperparams = {\n                    \"seed\": [1, 2, 3, 4, 5],\n                    \"embedding_sizes\": [24],\n                    \"sparsity_parameters\": [0],\n                    \"learning_rates\": [0.001],\n                    \"batch_sizes\": [32],\n                    \"betas\": [0.0005],\n                    \"lambdas\": [0.001],\n                    \"norm_regularizer_masking_opts\": [True],\n                    \"hypernetworks_hidden_layers\": [[100,100]],\n                    \"best_model_selection_method\": \"last_model\",\n                    \"saving_folder\": \"./Results/permuted_mnist_best_hyperparams/\",\n                }\n\n            # Both in the grid search and individual runs\n            hyperparams[\"lr_scheduler\"] = False\n            hyperparams[\"number_of_iterations\"] = 10000\n            hyperparams[\"number_of_epochs\"] = 10\n            hyperparams[\"no_of_validation_samples\"] = 5000\n            hyperparams[\"no_of_validation_samples_per_class\"] = 500\n            hyperparams[\"target_hidden_layers\"] = [256,256, 10]\n            hyperparams[\"target_network\"] = \"epsMLP\"\n            hyperparams[\"resnet_number_of_layer_groups\"] = None\n            hyperparams[\"resnet_widening_factor\"] = None\n            hyperparams[\"optimizer\"] = \"adam\"\n            hyperparams[\"chunk_size\"] = 100\n            hyperparams[\"chunk_emb_size\"] = 8\n            hyperparams[\"use_chunks\"] = False\n            hyperparams[\"adaptive_sparsity\"] = True\n            hyperparams[\"use_batch_norm\"] = False\n            # Directly related to the MNIST dataset\n            hyperparams[\"padding\"] = 2\n            hyperparams[\"shape\"] = (28 + 2 * hyperparams[\"padding\"]) ** 2\n            hyperparams[\"number_of_tasks\"] = 10\n            hyperparams[\"augmentation\"] = False\n\n    elif dataset == \"CIFAR100\":\n        if grid_search:\n            hyperparams = {\n                \"seed\": [5],\n                \"sparsity_parameters\": [0],\n                \"embedding_sizes\": [48],\n                \"betas\": [0.01, 0.05, 0.1, 1],\n                \"lambdas\": [0.01, 0.1, 1],\n                \"learning_rates\": [0.001],\n                \"batch_sizes\": [32],\n                \"norm_regularizer_masking_opts\": [False],\n                \"hypernetworks_hidden_layers\": [[100]],\n                \"resnet_number_of_layer_groups\": 3,\n                \"resnet_widening_factor\": 2,\n                \"optimizer\": \"adam\",\n                \"use_batch_norm\": True,\n                \"target_network\": \"ResNet\",\n                \"use_chunks\": False,\n                \"number_of_epochs\": 200,\n                \"augmentation\": True,\n            }\n            if part == 0:\n                pass\n            elif part == 1:\n                hyperparams[\"target_network\"] = \"ZenkeNet\"\n                hyperparams[\"resnet_number_of_layer_groups\"] = None\n                hyperparams[\"resnet_widening_factor\"] = None\n                hyperparams[\"use_chunks\"] = False\n                hyperparams[\"use_batch_norm\"] = False\n            else:\n                raise ValueError(f\"Wrong argument: {part}!\")\n            hyperparams[\n                \"saving_folder\"\n            ] = f\"./Results/grid_search/CIFAR_100_part_{part}/\"\n\n        else:\n            # Best hyperparameters for ResNet\n            hyperparams = {\n                \"seed\": [1, 2, 3, 4, 5],\n                \"embedding_sizes\": [48],\n                \"sparsity_parameters\": [0],\n                \"betas\": [0.01],\n                \"lambdas\": [1],\n                \"batch_sizes\": [32],\n                \"learning_rates\": [0.001],\n                \"norm_regularizer_masking_opts\": [False],\n                \"hypernetworks_hidden_layers\": [[100]],\n                \"use_batch_norm\": True,\n                \"use_chunks\": False,\n                \"resnet_number_of_layer_groups\": 3,\n                \"resnet_widening_factor\": 2,\n                \"number_of_epochs\": 200,\n                \"target_network\": \"ResNet\",\n                \"optimizer\": \"adam\",\n                \"augmentation\": True,\n            }\n            if part == 0:\n                # ResNet\n                pass\n            elif part == 1:\n                # ZenkeNet\n                hyperparams[\"lambdas\"] = [0.01]\n                hyperparams[\"target_network\"] = \"ZenkeNet\"\n                hyperparams[\"resnet_number_of_layer_groups\"] = None\n                hyperparams[\"resnet_widening_factor\"] = None\n            else:\n                raise ValueError(f\"Wrong argument: {part}!\")\n            hyperparams[\n                \"saving_folder\"\n            ] = f\"./Results/CIFAR_100_best_hyperparams_part_{part}/\"\n        hyperparams[\"lr_scheduler\"] = True\n        hyperparams[\"number_of_iterations\"] = None\n        hyperparams[\"no_of_validation_samples\"] = 500\n        hyperparams[\"no_of_validation_samples_per_class\"] = 50\n        if hyperparams[\"target_network\"] in [\"ResNet\", \"ZenkeNet\"]:\n            hyperparams[\"shape\"] = 32\n            hyperparams[\"target_hidden_layers\"] = None\n        elif hyperparams[\"target_network\"] == \"MLP\":\n            hyperparams[\"shape\"] = 3072\n            hyperparams[\"target_hidden_layers\"] = [1000, 1000]\n        hyperparams[\"number_of_tasks\"] = 10\n        hyperparams[\"chunk_size\"] = 100\n        hyperparams[\"chunk_emb_size\"] = 32\n        hyperparams[\"adaptive_sparsity\"] = True\n        hyperparams[\"padding\"] = None\n        hyperparams[\"best_model_selection_method\"] = \"val_loss\"\n\n    elif dataset == \"TinyImageNet\":\n        if grid_search:\n            hyperparams = {\n                \"seed\": [5],\n                \"sparsity_parameters\": [0, 30],\n                \"embedding_sizes\": [48],\n                \"betas\": [0.01, 0.1],\n                \"lambdas\": [0.01, 0.1],\n                \"learning_rates\": [0.001],\n                \"batch_sizes\": [16],\n                \"norm_regularizer_masking_opts\": [False],\n                \"hypernetworks_hidden_layers\": [[10, 10], [100]],\n                \"resnet_number_of_layer_groups\": 3,\n                \"resnet_widening_factor\": 2,\n                \"optimizer\": \"adam\",\n                \"use_batch_norm\": True,\n                \"target_network\": \"ResNet\",\n                \"use_chunks\": False,\n                \"number_of_epochs\": 10,\n                \"augmentation\": True,\n                \"saving_folder\": f\"./Results/TinyImageNet_grid_search_part_{part}/\",\n            }\n            if part == 0:\n                pass\n            elif part in [1, 2, 3]:\n                # ZenkeNet\n                hyperparams[\"seed\"] = [6]\n                hyperparams[\"target_network\"] = \"ZenkeNet\"\n                hyperparams[\"resnet_number_of_layer_groups\"] = None\n                hyperparams[\"resnet_widening_factor\"] = None\n                hyperparams[\"use_batch_norm\"] = False\n                hyperparams[\"hypernetworks_hidden_layers\"] = [[100, 100]]\n                hyperparams[\"learning_rates\"] = [0.001]\n                if part == 1:\n                    hyperparams[\"betas\"] = [0.01, 0.1, 1.0]\n                    hyperparams[\"lambdas\"] = [0.01, 0.1, 1.0]\n                    hyperparams[\"sparsity_parameters\"] = [0, 50, 70]\n                    hyperparams[\"embedding_sizes\"] = [96]\n                    hyperparams[\"learning_rates\"] = [0.005]\n                elif part == 2:\n                    hyperparams[\"embedding_sizes\"] = [128]\n                    hyperparams[\"betas\"] = [0.01, 0.1, 1.0]\n                    hyperparams[\"lambdas\"] = [0.01, 0.1, 1.0]\n                    hyperparams[\"sparsity_parameters\"] = [0, 30, 50]\n                elif part == 3:\n                    hyperparams[\"embedding_sizes\"] = [192]\n                    hyperparams[\"sparsity_parameters\"] = [0, 50]\n                    hyperparams[\"betas\"] = [0.001, 0.01, 0.1, 1]\n                    hyperparams[\"lambdas\"] = [0.001, 0.01, 0.1, 1]\n            else:\n                raise ValueError(f\"Wrong argument: {part}!\")\n            hyperparams[\n                \"saving_folder\"\n            ] = f\"./Results/Tiny_Zenke_grid_search_part_{part}/\"\n        else:\n            # ResNet\n            hyperparams = {\n                \"seed\": [5, 6, 7, 8, 9],\n                \"embedding_sizes\": [96],\n                \"sparsity_parameters\": [0],\n                \"betas\": [1],\n                \"lambdas\": [0.1],\n                \"batch_sizes\": [16],\n                \"learning_rates\": [0.0001],\n                \"norm_regularizer_masking_opts\": [False],\n                \"hypernetworks_hidden_layers\": [[100, 100]],\n                \"use_batch_norm\": True,\n                \"use_chunks\": False,\n                \"resnet_number_of_layer_groups\": 3,\n                \"resnet_widening_factor\": 2,\n                \"number_of_epochs\": 10,\n                \"target_network\": \"ResNet\",\n                \"optimizer\": \"adam\",\n                \"augmentation\": True,\n                \"saving_folder\": \"./Results/TinyImageNet/ResNet_best_hyperparams/\",\n            }\n            if part == 0:\n                pass\n            # ZenkeNet\n            elif part == 1:\n                hyperparams[\"target_network\"] = \"ZenkeNet\"\n                hyperparams[\"resnet_number_of_layer_groups\"] = None\n                hyperparams[\"resnet_widening_factor\"] = None\n                hyperparams[\"use_batch_norm\"] = False\n                hyperparams[\"sparsity_parameters\"] = [50]\n                hyperparams[\"betas\"] = [0.01]\n                hyperparams[\"lambdas\"] = [1.0]\n                hyperparams[\"learning_rates\"] = [0.001]\n                hyperparams[\n                    \"saving_folder\"\n                ] = f\"./Results/TinyImageNet/ZenkeNet_best_hyperparams/\"\n            else:\n                raise ValueError(f\"Wrong argument: {part}!\")\n        hyperparams[\"lr_scheduler\"] = True\n        hyperparams[\"number_of_iterations\"] = None\n        hyperparams[\"no_of_validation_samples\"] = 250\n        hyperparams[\"no_of_validation_samples_per_class\"] = 50\n        if hyperparams[\"target_network\"] in [\"ResNet\", \"ZenkeNet\"]:\n            hyperparams[\"shape\"] = 64\n            hyperparams[\"target_hidden_layers\"] = None\n        elif hyperparams[\"target_network\"] == \"MLP\":\n            hyperparams[\"shape\"] = 12288\n            hyperparams[\"target_hidden_layers\"] = [1000, 1000]\n        hyperparams[\"number_of_tasks\"] = 40\n        hyperparams[\"chunk_size\"] = 100\n        hyperparams[\"chunk_emb_size\"] = 32\n        hyperparams[\"adaptive_sparsity\"] = True\n        hyperparams[\"padding\"] = None\n        hyperparams[\"best_model_selection_method\"] = \"val_loss\"\n\n    elif dataset == \"SplitMNIST\":\n        if grid_search:\n            hyperparams = {\n                \"learning_rates\": [0.001],\n                \"batch_sizes\": [128],\n                \"norm_regularizer_masking_opts\": [False],\n                \"betas\": [0.001],\n                \"hypernetworks_hidden_layers\": [[25, 25]],\n                \"sparsity_parameters\": [0],\n                \"lambdas\": [0.001],\n                # Seed is not for optimization but for ensuring multiple results\n                \"seed\": [1, 2, 3, 4, 5],\n                \"best_model_selection_method\": \"last_model\",\n                \"embedding_sizes\": [128],\n                \"augmentation\": True,\n            }\n            if part == 0:\n                pass\n            elif part == 1:\n                hyperparams[\"embedding_sizes\"] = [96]\n                hyperparams[\"hypernetworks_hidden_layers\"] = [[50, 50]]\n                hyperparams[\"betas\"] = [0.01]\n                hyperparams[\"sparsity_parameters\"] = [30]\n                hyperparams[\"lambdas\"] = [0.0001]\n            elif part == 2:\n                hyperparams[\"sparsity_parameters\"] = [30]\n                hyperparams[\"norm_regularizer_masking_opts\"] = [True]\n            else:\n                raise ValueError(\"Not implemented subset of hyperparameters!\")\n\n            hyperparams[\"saving_folder\"] = \"./Results/grid_search/split_mnist/\"\n\n        else:\n            # Best hyperparameters\n            hyperparams = {\n                \"seed\": [1],\n                \"embedding_sizes\": [128],\n                \"sparsity_parameters\": [30],\n                \"learning_rates\": [0.001],\n                \"batch_sizes\": [128],\n                \"betas\": [0.001],\n                \"lambdas\": [0.001],\n                \"norm_regularizer_masking_opts\": [True],\n                \"hypernetworks_hidden_layers\": [[25, 25]],\n                \"augmentation\": True,\n                \"best_model_selection_method\": \"last_model\",\n                \"saving_folder\": \"./Results/split_mnist_test/\",\n            }\n        hyperparams[\"lr_scheduler\"] = False\n        hyperparams[\"target_network\"] = \"epsMLP\"\n        hyperparams[\"resnet_number_of_layer_groups\"] = None\n        hyperparams[\"resnet_widening_factor\"] = None\n        hyperparams[\"optimizer\"] = \"adam\"\n        hyperparams[\"number_of_iterations\"] = 200\n        hyperparams[\"number_of_epochs\"] = None\n        hyperparams[\"no_of_validation_samples\"] = 1000\n        hyperparams[\"no_of_validation_samples_per_class\"] = 100\n        hyperparams[\"target_hidden_layers\"] = [400, 400]\n        hyperparams[\"shape\"] = 28**2\n        hyperparams[\"number_of_tasks\"] = 5\n        hyperparams[\"chunk_size\"] = 100\n        hyperparams[\"chunk_emb_size\"] = 96\n        hyperparams[\"use_chunks\"] = False\n        hyperparams[\"use_batch_norm\"] = False\n        hyperparams[\"adaptive_sparsity\"] = True\n        hyperparams[\"padding\"] = None\n\n    elif dataset == \"CIFAR100_FeCAM_setup\":\n        if grid_search:\n            hyperparams = {\n                \"seed\": [1],\n                \"sparsity_parameters\": [0],\n                \"betas\": [0.01],\n                \"lambdas\": [1],\n                \"batch_sizes\": [32],\n                \"norm_regularizer_masking_opts\": [False],\n                \"hypernetworks_hidden_layers\": [[100]],\n                \"use_batch_norm\": True,\n                \"use_chunks\": False,\n                \"resnet_number_of_layer_groups\": 3,\n                \"resnet_widening_factor\": 2,\n                \"number_of_epochs\": 200,\n                \"target_network\": \"ResNet\",\n                \"optimizer\": \"adam\",\n                \"augmentation\": True,\n            }\n            if part == 0:\n                pass\n            elif part == 1:\n                hyperparams[\"embedding_sizes\"] = [24, 48, 96]\n                hyperparams[\"learning_rates\"] = [0.0001, 0.001, 0.01]\n                hyperparams[\"hypernetworks_hidden_layers\"] = [[100], [200]]\n                hyperparams[\"number_of_tasks\"] = 5\n            hyperparams[\n                \"saving_folder\"\n            ] = f\"./Results/CIFAR_100_FeCAM_setup_part_{part}/\"\n        else:\n            # Best hyperparameters for ResNet\n            hyperparams = {\n                \"seed\": [1, 2, 3, 4, 5],\n                \"embedding_sizes\": [48],\n                \"sparsity_parameters\": [0],\n                \"betas\": [0.01],\n                \"lambdas\": [1],\n                \"batch_sizes\": [32],\n                \"learning_rates\": [0.0001],\n                \"norm_regularizer_masking_opts\": [False],\n                \"hypernetworks_hidden_layers\": [[200]],\n                \"use_batch_norm\": True,\n                \"use_chunks\": False,\n                \"resnet_number_of_layer_groups\": 3,\n                \"resnet_widening_factor\": 2,\n                \"number_of_epochs\": 200,\n                \"target_network\": \"ResNet\",\n                \"optimizer\": \"adam\",\n                \"augmentation\": True,\n            }\n            # FeCAM considered three incremental scenarios: with 6, 11 and 21 tasks\n            # ResNet - parts 0, 1 and 2\n            # ZenkeNet - parts 3, 4 and 5\n            # Also, one scenario with equal number of classes: ResNet - part 6\n            if part in [0, 3]:\n                hyperparams[\"number_of_tasks\"] = 6\n            elif part in [1, 4]:\n                hyperparams[\"number_of_tasks\"] = 11\n            elif part in [2, 5]:\n                hyperparams[\"number_of_tasks\"] = 21\n            elif part in [6, 7]:\n                hyperparams[\"number_of_tasks\"] = 5\n            if part in [3, 4, 5, 7]:\n                hyperparams[\"lambdas\"] = [0.01]\n                hyperparams[\"target_network\"] = \"ZenkeNet\"\n                hyperparams[\"resnet_number_of_layer_groups\"] = None\n                hyperparams[\"resnet_widening_factor\"] = None\n            if part not in [0, 1, 2, 3, 4, 5, 6, 7]:\n                raise ValueError(f\"Wrong argument: {part}!\")\n            hyperparams[\n                \"saving_folder\"\n            ] = f\"./Results/CIFAR_100_FeCAM_part_{part}/\"\n        hyperparams[\"lr_scheduler\"] = True\n        hyperparams[\"number_of_iterations\"] = None\n        hyperparams[\"no_of_validation_samples_per_class\"] = 50\n        if hyperparams[\"target_network\"] in [\"ResNet\", \"ResNetF\", \"ZenkeNet\"]:\n            hyperparams[\"shape\"] = 32\n            hyperparams[\"target_hidden_layers\"] = None\n        elif hyperparams[\"target_network\"] == \"MLP\":\n            hyperparams[\"shape\"] = 3072\n            hyperparams[\"target_hidden_layers\"] = [1000, 1000]\n        hyperparams[\"chunk_size\"] = 100\n        hyperparams[\"chunk_emb_size\"] = 32\n        hyperparams[\"adaptive_sparsity\"] = True\n        hyperparams[\"padding\"] = None\n        hyperparams[\"best_model_selection_method\"] = \"val_loss\"\n\n    else:\n        raise ValueError(\"This dataset is not implemented!\")\n\n    # General hyperparameters\n    hyperparams[\"activation_function\"] = torch.nn.ELU()\n    hyperparams[\"norm\"] = 1  # L1 norm\n    hyperparams[\"use_bias\"] = True\n    hyperparams[\"save_consecutive_masks\"] = False\n    hyperparams[\"device\"] = \"cpu\"\n    hyperparams[\"dataset\"] = dataset\n    os.makedirs(hyperparams[\"saving_folder\"], exist_ok=True)\n    return hyperparams\n\n\nif __name__ == \"__main__\":\n    datasets_folder = \"./Data\"\n    os.makedirs(datasets_folder, exist_ok=True)\n    validation_size = 500\n    use_data_augmentation = False\n    use_cutout = False\n\n    split_cifar100_list = prepare_split_cifar100_tasks(\n        datasets_folder, validation_size, use_data_augmentation, use_cutout\n    )\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/datasets.py b/datasets.py
--- a/datasets.py	(revision 9bcf188f17f73fe837356539a1ef3df5f2cd5485)
+++ b/datasets.py	(date 1739547276598)
@@ -622,7 +622,7 @@
                 "saving_folder": "./Results/split_mnist_test/",
             }
         hyperparams["lr_scheduler"] = False
-        hyperparams["target_network"] = "epsMLP"
+        hyperparams["target_network"] = "MLP"
         hyperparams["resnet_number_of_layer_groups"] = None
         hyperparams["resnet_widening_factor"] = None
         hyperparams["optimizer"] = "adam"
